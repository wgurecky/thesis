%! TEX root = ../dissertation_gurecky.tex

\section{Model Approach}

A fundamental difference between the CFD and CTF computations is the average size of the mesh cells.  In the azimuthal coordinate, CTF decomposes a single rod surface into four patches.  An example top down view of typical CFD and CTF meshes for a single pin are given in figure \ref{fig:cfd_ctf_mesh}.  Though both codes employ a finite volume spatial discretization CFD can resolve the flow at much smaller length scales.  Additionally, each code employs a different set of closure models to the underlying set of coupled energy, mass, and momentum balances.  In practice these differences can lead to large discrepancies in boiling, turbulent mixing, and rod surface temperature predictions between the two codes.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=10cm]{../proposal/images/cfd_ctf_mesh.png}
    \caption{Top-down view of typical subchannel and CFD meshes for a single pin \cite{salko12}.}
    \label{fig:cfd_ctf_mesh}
\end{figure}

Shown in figure \ref{fig:model_overview}, on a given CTF rod surface patch, a single point estimates for the surface temperature, TKE, and heat flux are predicted.  The predicted CTF quantities are an estimate for the average thermal hydraulic conditions over that coarse patch.   Consequentially, CTF CRUD predictions may significantly deviate from reality.  Since CRUD growth is highly sensitive to the presence of subcooled boiling on the rod surface; if CTF predicts a rod surface temperature less than the saturation point very little or no CRUD will form - when in reality, a small portion of that rod surface could exist above the saturation point and thus harbor CRUD.  Small localized mistakes in CRUD predictions compound throughout the core, leading to poor CIPS estimates. 

In figure \ref{fig:model_overview}, $f$ denotes a probability density function.  Integration of this density function can be interpreted as computing a fractional area of the rod surface that exists within the specified integration limits.  
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=12cm]{../proposal/images/model_relations.png}
    \caption{On a single coarse CTF patch: Differences in CRUD prediction between CFD and CTF models.}
    \label{fig:model_overview}
\end{figure}

CTF estimates mean TH conditions everywhere in the core at a low spatial resolution.  The CFD informed model provides higher order moments about the mean.
\begin{equation}
   \mathbf S(\mathbf p, \mathbf z) = \underbrace{ \bm \mu(\mathbf p, \mathbf{z})}_\text{CTF} +
   \underbrace{\varepsilon({\theta (\bm p, \mathbf z)})}_\text{CFD Informed} + \bm b(\mathbf p, \mathbf{z})
   \label{eq:hi2lo_overview}
\end{equation}

\begin{itemize}
        \item $\mathbf S$ is a three component vector representing the cladding surface temperature, turbulent kinetic energy and boundary heat flux.
        \item $\mathbf z$ denotes spatial coordinates and $\mathbf p$ represents a set of auxiliary predictors.  Auxiliary predictors are covariates that describe local core conditions and may be geometric or thermal hydraulic in nature.
        \item $\varepsilon$ is a random three-component vector comprised of temperature, turbulent kinetic energy and boundary heat flux fields: $\{\varepsilon_T, \varepsilon_k, \varepsilon_{q''}\}$.  $\varepsilon$ is a CFD informed model with $\theta$ representing free model parameters which must be determined given CFD data.
        \item $\bm b$ is bias between the low and high fidelity models ($\bm \mu_{CTF} - \bm \mu_{CFD}$).  Despite providing identical inlet boundary conditions to both codes bias exists due to algorithmic differences, closure model differences, and meshing differences between the two codes.
        \item Field averages, $\bm \mu$, are piecewise constant over each CTF patch.
        \item Note this is a additive model where Salko constructed a multiplicative model.
\end{itemize}

Consider the hypothetical case where the CFD results are normally distributed about the CTF results such that $\varepsilon \sim \mathcal N(0, \mathbf \theta(\mathbf p, \mathbf z))$, where $\mathbf \theta(\mathbf p, \mathbf z) = \bm \Sigma(\mathbf p, \mathbf z)$ is a covariance matrix that depends on local core conditions.  Shifting this distribution by a constant vector $\bm c=\bm b + \bm \mu_{ctf}$, results in a new distribution denoted by $h$ in equation \ref{eq:norm_noise}.
\begin{align}
    \left. h \right|_{(\bm p, \bm z)} & = \mathcal N(\bm c, \bm \Sigma(\mathbf p, \bm z)) \nonumber \\
    & = \left.
        \mathcal N \left(
        \begin{pmatrix}
            c_T \\
            c_k \\
            c_{q''}
        \end{pmatrix}
    ,
        \begin{pmatrix}
            \sigma_{T} \sigma_{T} & \sigma_{T} \sigma_{k} & \sigma_{T} \sigma_{q''} \\
            \sigma_{k} \sigma_{T} & \sigma_{k} \sigma_{k} & \sigma_{k} \sigma_{q''} \\
            \sigma_{q''} \sigma_{T} & \sigma_{q''} \sigma_{k} & \sigma_{q''} \sigma_{q''}
        \end{pmatrix}
    \right)
    \right|_{(\mathbf p, \mathbf z)}
\label{eq:norm_noise}
\end{align}


The goal is to estimate the expected crud mass $C_m$ on each CTF patch given by equation \ref{eq:expected_crud}.  Let $\mathbf X= \{T, k, q''\}$ denote a random vector of temperature, TKE, and BHF. $\mathbf I$ represents additional known crud parameters, $\mathbf C_o$ is the crud state at the start of the time step and $\theta$ are distribution parameters.    The CRUD model, $\mathcal G(\cdot)$, is common to all CTF faces.  The joint cumulative density's parameters, $\theta$, $(\hat H(\cdot|\theta))$ must be predicted from the available high resolution CFD data in every CTF face.  In the case of an assumed normal distribution model there are nine unknowns:  $\{ \sigma^2_T, \sigma^2_k, \sigma^2_{q''}, \sigma_k \sigma_T, \sigma_{q''}\sigma_T, \sigma_{q''}\sigma_{k}, c_T, c_k, c_{q''} \}$.  In the subsequent sections we will seek to relax this normality assumption of the CFD residuals about the CTF result.

\begin{align}
        C_m &= A \mu_{m} \nonumber \\
        &= A \E[\mathcal G(\mathbf X|\mathbf C_o, \mathbf I, \delta t)] \nonumber \\
        &= A \iiint \mathcal G(\mathbf X|\mathbf C_o, \mathbf I, \delta t) h(\mathbf X|\theta) d \mathbf X
        \label{eq:expected_crud}
\end{align}

A strategy to compute the unknowns of the join distribution on each CTF face is required.  In this work we propose a data driven model, $\mathcal M$, to predict the unknowns provided a suite of pre-computed CFD results are used to train the machine learning model and local thermal hydraulic conditions provided by CTF at runtime are utilized to evaluate the model on each CTF face. Algorithm \ref{algo:basic_crud_algo} is used to compute the total crud mass in each CTF face.

\begin{algorithm}[H]
    \captionsetup{labelfont={sc,bf}, labelsep=newline}
    \caption{Generic hi2lo method for crud prediction.}
    \begin{algorithmic}      
    \STATE \textbf{Initialization}  
    \STATE (1) Pre-process training set.  
    \STATE $\ \ $   (1b) Fit the joint distribution parameters to known CFD data: $\theta(\mathbf p, \mathbf z)$.  
    \STATE $\ \ $   (1c) \textbf{def:}  $\ \theta \leftarrow \mathcal M(\mathbf p, \mathbf z | \gamma)$
    \STATE (2) Train model:  $\hat \gamma =  argmin_\gamma ||\mathcal M(\mathbf p, \mathbf z| \gamma) - \theta(\mathbf p, \mathbf z)|| $
\FOR {CTF face, $j$}
    \STATE Evaluate ML model $\hat \theta_j \leftarrow \mathcal M(\mathbf p_{j}, \mathbf z_j; \hat \gamma)$ \;
    \STATE Reconstruct $\hat H_j(\cdot |\hat \theta_j)$ \;
    \STATE Draw samples $\mathbf X \sim \hat H_j$ \;
    \STATE Evaluate \ref{eq:expected_crud} via Monte Carlo approximation \;
\ENDFOR
    \end{algorithmic}
\label{algo:basic_crud_algo}
\end{algorithm}
Where $\gamma$ are free parameters in the machine learning regression model.  A discussion on the machine learning model follows in section ().  The reconstruction of the joint density function $\hat H$ from copula and univariate quantile functions is discussed in section ().  Monte Carlo and importance sampling are discussed in sections () and () respectively.  A critical question to answer is:  Can a data driven machine learning model adequately predict the parameters of a probability distribution on each CTF face: $Var[(\hat \theta_j \leftarrow \mathcal M(\mathbf p_{j}, \gamma))] < \epsilon$?  There is some uncertainty in the optimal machine learning model parameters.  This propagates to the distribution parameters which ultimately governs the uncertainty of the crud results.  An adequet model in this contex reduces the uncertainty in the final crud estimate to some acceptable level.
\bigskip

In addition to improving the expected value prediction of CRUD on each CTF patch vs the CTF standalone case, the model provides the capability to estimate the likelihood of extreme value events (i.e. $\mathcal P_f \propto Pr(C_t(x) > C_t^*)$, where $C_t^*$ is some critical crud thickness and $\mathcal P_f$ is a cladding failure probability) on the rod surface.  This would be impossible to quantify with CTF/MAMBA alone.  A significant challenge is computing an estimate for $Var(\mathcal P_f) = \E[(\mathcal P_f - \E(\mathcal P_f))^2]$.


\section{Construction of the Hi2lo Map}

\subsection{Capturing Dependence Between Random Variables}

Since the outer cladding temperature, near-wall TKE and boundary heat flux are used as boundary conditions to a crud growth package, it is particularly important to understand and capture the relationship between these fields in the hi2lo model.  The hi2lo model under consideration is not a dynamic model in the sense it is not a coupled system of differential equations.  Instead, in the purely data driven approach the relationships are established through standard statistical correlation measures.

The coupled momentum, mass, and thermal balance in a core simulation along with the appropriate closure models dictate the rod outer cladding surface temperature.  The Dittus Boelter relationship is a commonly used closure model which relates the surface heat transfer coefficient with the local Reynolds number.  According to this relationship larger Reynolds numbers corresponds to higher heat transfer coefficients.   Newtons law of cooling states $T_s = q''/h + T_{\infty}$ and $h$ may be computed via Dittus Boelter (See Appendix D).  Therefore, the surface temperature $(T_s)$ is negatively correlated with the Reynolds number and where the local turbulent kinetic energy is large the rod surface temperature will be depressed if the local heat flux is held fixed.  It is also apparent that the surface temperature is positively correlated with the the local boundary heat flux.  In order to simplify the model only the dependency between the surface temperature and local turbulent kinetic energy is considered in hi2lo model.

\textbf{Assume}:
\begin{itemize}
\item Temperature and TKE are uncorrelated with the boundary heat flux.
\end{itemize}

\textbf{Justification}:
        \begin{itemize}
                \item  Relative variations in boundary heat flux are very small over a CTF face $(+/- 5\%)$.  Recall a CTF face represents a small approximate $1cm^2$ patch on the rod surface.  Large absolute heat fluxs of approximately $80W/cm^2$ are possible on the cladding surface of fuel in a typical PWR.  However, high axial and azimulthal gradients of boundary heat flux on the rod surface are not typical under standard operation conditions.  High thermal gradients can lead to extra thermal stresses on the cladding which can promote rod bowing or extreme in conditions fuel failure.
                \item Figures \ref{fig:crud_sensi1} to \ref{fig:crud_sensi3} show that the sensitivity of crud growth rate to the boundary heat flux is small relative to sensitivity of curd to surface temperature and local turbulent kinetic energy. 
        \end{itemize}

After applying the independence assumptions the original trivariate dependence model between the temperature, TKE, and boundary heat flux can be reduced to a bivariate model with the boundary heat flux being treated and sampled from independently.

Vine copula provide a flexible framework to model high dimensional dependence structures \cite{Joe2015}. Vine copula are hierarchical tree models in which the edges represent bivariate copula and the nodes are univariate distributions. Vine copula have seen significant historical use in the financial industry in portfolio value at risk models.  The canonical vine copula shown in equation \ref{eq:ori_vine_model} can be used to express the trivariate distribution of temperature, TKE, and $q''$ on the rod surface.
\begin{equation}
h(T, k, q'') = f_T f_k, f_{q''} \prod_c c_i(u_i, v_i)
\label{eq:ori_vine_model}
\end{equation}
Where each bivariate copula model in $\prod c_i $ is known as a pair copula.  The complete nested tree model is sometimes refereed to as a pair copula construction in the literature.

Applying the simplifying assumptions results in: $c(u_T, u_{q''}) = 1$ and $c(u_{k}, u_{q''}) = 1$. The simplified joint density can then be given by equation \ref{eq:simple_vine_model}:
\begin{equation}
h(T, k, q'') \approx  f_T f_k, f_{q''} c(u_{T}, v_{k})  \cdot 1 \cdot 1
\label{eq:simple_vine_model}
\end{equation}

\input{./sections/appendix/app_a.tex}


\subsection{Sample Quantiles}

\subsubsection{Non Parametric Representations of Univariate Distributions}

\begin{itemize}
    \item (\checkmark-) Introduce non-parametric representation of univariate distributions through sample quantiles.
    \item ($\cdot$) Explain typical alternatives and why they are not suitable:
        \begin{itemize}
            \item Compute sample moments.  Use method of moments to fit a model to sample moments.
            \item Compute sample cumulants.  Use sample cumulants to build an Edgeworth series.
            \item Do cumulants or traditional moments behave in a predictable manner as a function of local core conditions?  Do the quantiles behave in a predictable manner?
        \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{../proposal/slides/seminar_slides/figs/margins_cdf_2}
    \caption[CDF from quantiles.]{Piecewise linear CDF interpolated from a set of quantiles.}
    \label{fig:marginscdf2}
\end{figure}


The $\tau^{th}$ quantile is $q_\tau = F^{-1}(\tau); $ where $\ F(t)=P[T \leq t]$.
$\tau \in [0, 1]$

The quantile loss function is given by equation \ref{eq:qloss_fn}.
\begin{equation}
\rho_\tau( u) = \mathbf u \cdot (\tau - \mathbb{I}_{( u < 0)})
\label{eq:qloss_fn}
\end{equation}
Where $\mathbb{I}$ is the indicator function which returns 1 if the argument is true and 0 otherwise.
In order to estimate a sample quantile, $\hat q_\tau$, given the empirical CDF $F$, minimize: $\E[\rho_\tau(T - q_\tau)]$ where $T$ is a random variable distributed according to $F$.
\begin{equation}
            \left.\begin{aligned}
            \hat q_{\tau_i} &= argmin_{q_\tau} \E[\rho(u)];\ \  u = T - q_\tau  \\
            \approx & argmin_{q_\tau}  \frac{1}{N} \sum_i^N \rho(u_i); \ u_i = t_i - q_\tau \\
            \approx & argmin_{q_\tau} \left[ (1-\tau) \sum_{t \leq q_\tau}( t_i - q_\tau ) - \tau \sum_{t > q_\tau} (t_i - q_\tau) \right]
            \end{aligned}\right.
\end{equation}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{../proposal/slides/seminar_slides/figs/q_loss}
    \caption[Quantile loss function.]{Quantile loss function.}
    \label{fig:qloss}
\end{figure}



\begin{itemize}
        \item (\checkmark) Compute sample quantiles.
        \item (\checkmark) Building a cumulative density function from sample quantiles.
        \begin{itemize}
           \item Piecewise linear CDF leads to histogram PDF.
           \item PCHIP spline CDF preserves monotonicity of CDF and results in a smooth PDF. \cite{Fritsch80}
        \end{itemize}
        \item (\checkmark) Sampling from a known CDF via inverse transform sampling.
        \item (\checkmark-) Show that the order statistics are distributed according to a Gaussian distribution.
        \item (\checkmark-) Show that sample quantiles also follow a Gaussian distribution.  This directly follows from the distribution of the order statistics.
            The sample quantiles are distributed normally according to equation \ref{eq:theory_qdist_1}.
        \begin{eqnarray}
        q_p &\sim \mathcal N \left( F_T^{-1}(p), \sigma^2_{q_p} \right) \\
        \sigma^2_{q_p} &= \frac{p(1 - p)}{n[f_T(F_T^{-1}(p))]^2}
        \label{eq:theory_qdist_1}
        \end{eqnarray}
        


\begin{figure}[]%
    \centering
    \subfloat[Emperical quantiles for a normal distrubtion. 500 trials shown. $0.1, 0.5, 0.9$ quantiles denoted by dashed verticle lines.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual} }}%
    \qquad
        \subfloat[Emperical vs theoredical $0.1$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual_conditional_0_1} }}%
    \qquad
        \subfloat[Emperical vs theoredical $0.5$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual_conditional_0_5} }}%
    \qquad
        \subfloat[Emperical vs theoredical $0.9$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual_conditional_0_9} }}%
    \qquad
    \caption[Distribution of quantiles for a normally distributed RV.]{Distribution of quantiles for a normally distributed RV: $X \sim \mathcal{N}(0, 1)$.  Theoretical quantile standard deviation given by equation \ref{eq:theory_qdist_1}.}%
    \label{fig:normal_q_theory}%
\end{figure}

\begin{figure}[H]%
    \centering
    \subfloat[Emperical quantiles for a beta distrubtion. 500 trials shown. $0.1, 0.5, 0.9$ quantiles denoted by dashed verticle lines.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual} }}%
    \qquad
    \subfloat[Emperical vs theoredical $0.1$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual_conditional_0_1} }}%
    \qquad
    \subfloat[Emperical vs theoredical $0.5$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual_conditional_0_5} }}%
    \qquad
    \subfloat[Emperical vs theoredical $0.9$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual_conditional_0_9} }}%
    \qquad
    \caption[Distribution of quantiles for a beta distributed RV.]{Distribution of quantiles for a beta distributed RV: $X \sim \beta(1, 2)$}%
    \label{fig:ctf_twall_aug}%
\end{figure}


        \item ($\cdot$) Show the impact of the uncertainty in where the sample quantiles fall given CFD data on crud growth.  The reconstructed CDFs which comprise the hi2lo mapping are essentially fuzzy which means the temperature, TKE, and BHF distributions are artificially smeared out (artificially high densities in the tails).
\end{itemize}

\begin{itemize}
    \item (\checkmark-) The sampled temperatures may be tallied over each CTF face to estimate the fractional area that exceeds some threshold temperature.
    The probability of exceeding a threshold temperature is shown in equation \ref{eq:pr_thresh}.
    
    \begin{equation}
    p_e = Pr(T > T^*) = 1 - \int_0^{T^*} f_T dT
    \label{eq:pr_thresh}
    \end{equation}
    
    Let $q_p = F_T^{-1}(1 - p_e)$
    denote the quantile associated with the threshold probability, $p_e$.
    $F_T^{-1}$ is the inverse CDF function and $f_T$ is the probability density function of temperature on the patch.
    
    The sample quantile corresponding to $p_e$ is distributed according to:

   \begin{eqnarray}
    q_p &\sim \mathcal N \left( F_T^{-1}(p), \sigma^2_{q_p} \right) \\
    \sigma^2_{q_p} &= \frac{p(1 - p)}{n[f_T(F_T^{-1}(p))]^2}
    \end{eqnarray}
    
    The variance of upper tail probability mass estimate can be found by standard propagation of uncertainty principles:
   \begin{equation}
    \sigma_p^2 = \left(\frac{\partial p_e}{\partial q_p} \right)^2 \cdot \sigma_{q_p}^2 +\  HOT.
    \end{equation}
    
    Where
   \begin{eqnarray}
    \frac{\partial p_e}{\partial q_p} &= \frac{\partial}{\partial q_p} \left( 1 - \int_0^{q_p} f_T dT \right) \nonumber \\
    &= \frac{\partial}{\partial q_p} \left( -F_T(q_p) + F_T(0) \right) \nonumber \\
    &= -f_T(q_p)
    \end{eqnarray}
    
    This dictates that estimates of extreme upper tail integrals carry large relative uncertainties.
    \item The approximate large sample distribution for $p_e$ can be obtained by a change of variables, since $p_e$ and $q_p$ are related by:
    \begin{equation}
    q_p = F_T^{-1}(1 - p_e)
    \end{equation}
    
    Where $F_T^{-1}$ is the inverse CDF.  $p_e$ is distributed according to:
    
    \begin{equation}
    f_{p_e} = f_{q_p} \left( F_T^{-1}(1-p_e), \sigma_{q_p}(p_e) \right) \cdot \left|{ \frac{\partial}{\partial p_e} F^{-1}(1-p_e) } \right|
    \end{equation}
    
\end{itemize}


\subsection{Gradient Boosted Regression Trees}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{../proposal/slides/seminar_slides/figs/cart}
    \caption[Regression tree stump.]{Single CART Regression tree stump comparing a fit of depth 1 and 2.}
    \label{fig:cart}
\end{figure}

The generalized gradient boosting algorithm was developed by Friedman (1999).  This work was significant because it re-invisioned previous boosting algorithms such as adaBoost as specific special cases of general gradient boosting with specific loss functions.  Gradient boosting is a numerical optimization procedure in function space \cite{friedman2001}.  Friedman described this as finding a function $\mathcal M^*$ that maps the inputs $\mathbf x$ to $y$ where $\mathcal M^*$ is given by:

\begin{equation}
\mathcal M^* = \text{argmin}_F \E_{y, \mathbf x} (L(y, F(\mathbf x)))
\end{equation}
Where $L(\cdot)$ is a differentiable loss function.  For the standard gradient boosting trees the functional form of $F$ is chosen to be an additive model of the form \cite{friedman2001}:
\begin{equation}
F(\mathbf x; \mathbf{\beta}, \mathbf a) = \sum_{m=0}^M \beta_m h(\mathbf x; a_m)
\end{equation} 

This hi2lo work employs gradient tree boosting in which decision trees are greedily fit to the gradient of the loss function.  For clarity, in the literature the vector of gradients is sometimes referred to as the "pseudo residuals vector" since if the loss function is taken to be the L2 loss, the gradient of the loss functions is proportional to the residual vector.  

The  weak learner is defined to be a typical classification and regression tree (CART).  Methods for pruning the tree and optimizing the numerical implementation of finding the best splits are left to (refs).  Examples of single classification tree with a depth of 1 and 2 are shown in figure \ref{fig:cart}.  The decision tree produces a piecewise constant prediction.

\begin{algorithm}[H]
    \captionsetup{labelfont={sc,bf}, labelsep=newline}
    \caption{Gradient boosting algorithm \cite{friedman2002}.}
    \begin{algorithmic}
    \STATE \textbf{Initialization} 
    \STATE (1) Training set $\{(p_i, y_i)\}_{i=1}^n$. 
    \STATE (2) Differentiable loss function $L(y, F(p))$. 
    \STATE (3) Number of iterations ${{M}}$.
    \STATE (4)   Initialize model with a constant value:
        $F_0(p) = \underset{\gamma}{\arg\min} \sum_{i=1}^n L(y_i, \gamma).$
    
    \FOR {${{m}} = 1$ to ${{M}}$}
        \STATE Compute the pseudo-residuals:  
        \FOR {$i=1,\ldots,n $}
            \STATE $r_{im} = -\frac{\partial L(y_i, F_{m-1}(p_i))}{\partial F_{m-1}(p_i)}$
        \ENDFOR
        
        \STATE Fit a weak learner $h_m(p)$ to pseudo-residuals, $r_{m}$: Training data set is $\{(p_i, r_{im})\}_{i=1}^n$ \;
        
        \STATE Compute multiplier $\gamma_m$ :
        $\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, F_{m-1}(p_i) + \gamma h_m(p_i)\right)$\;
        \STATE Update the model:
        $F_m(p) = F_{m-1}(p) + \nu \gamma_m h_m(p).$
    
    \ENDFOR
    \STATE Output $F_M(p).$
    \end{algorithmic}
\label{alg:boosting}
\end{algorithm}
Where $\nu$ is a tunable constant in $[0, 1]$ called the learning rate.  The learning rate reduces the contribution of each weak learner in the final model.  Reducing the learning rate results in a proportional increase in the number of boosted iterations to achieve the same level of convergence of the boosted tree chain.  Smaller values for the learning rate result in performing smaller steps in function space so we are less likely to overshoot the optimal function - since this is a stage-wise additive model overshoots are problematic since the final model ``remembers`` mistakes made in previous iterations.  A typical learning rate is $\nu\leq 0.05$.

For quantile regression, the loss function is given by equation \ref{eq:qloss_fn} is substituted into algorithm \ref{alg:boosting}:
\begin{equation}
L(y, F(p);\tau) = \left[ (1-\tau) \sum_{y \leq F(p)}( y_i - F(p) ) \right] - \left[ \tau \sum_{y > F(p)} (y_i - F(p)) \right]
\end{equation}
Where $\tau$ is the user set probability level of interest. for instance if the 95\% quantile is desired then $\tau=0.95$.

% boosting tree results
\input{./sections/appendix/app_c.tex}

\subsection{Monte Carlo Crud Estimation}


First the goal is to reconstruct the joint density function from copula and marginal distributions.  The margins are defined by their quantile functions as shown in section ().  Equation () defines the joint density. After reconstruction of the joint temperature, boundary heat flux and turbulent kinetic energy distribution on each patch, one draws samples via methods outlined in section (), or in particular equation ().  This leads to the Monte Carlo approximation of integral shown previously in equation \ref{eq:expected_crud} shown in equation \ref{eq:mc_expected_crud}.

\begin{equation}
\E(\mathcal G(x)) \approx \frac{1}{N} \sum_i^N \mathcal G(x_i) h(x_i), \ x \sim {h}
\label{eq:mc_expected_crud}
\end{equation}

Rather than sampling from the density function $h$, one may draw samples from an alternate proposal density distribution denoted, $\tilde h$, and appropriately weight the samples by the proability ratio of the original density to the proposal density: $h/\tilde h$.  This leads to importance sampling formulation of equation \ref{eq:mc_expected_crud} which is given in equation \ref{eq:mc_imp_expected_crud}.

\begin{equation}
\E(\mathcal G(x)) \approx \frac{1}{N} \sum_i^N \mathcal G(x_i) \frac{h(x_i)}{\tilde h(x_i)}, \ x \sim \tilde{h}
\label{eq:mc_imp_expected_crud}
\end{equation}

Substituting the joint density function defined by \ref{eq:sklar2} in to () yields:

\begin{align}
    h(x_1, x_2..) &= c(u_1, u_2, ...) f(x_1) f(x_2) \nonumber \\
    \tilde h(x_1, x_2..) &= \tilde c(u_1, u_2, ...) \tilde f(x_1) \tilde f(x_2)
\label{eq:mc_imp_int_2}
\end{align}

\begin{equation}
\E(\mathcal G(x)) \approx \frac{1}{N} \sum_i^N \mathcal G_i(x) \omega_i(x), \ x \sim \tilde{f_1} \tilde f_2 \tilde c(u_1, u_2)
\label{eq:mc_imp_int2}
\end{equation}

with the probability ratio of the target density to the proposal given by \ref{eq:imp_prob_ratio}:

\begin{equation}
\omega_i = \frac{h_i}{\tilde h_i} = \frac{f_1(x_i) f_2(x_i)c}{\tilde f_1(x_i) \tilde f_2(x_i) \tilde c}
\label{eq:imp_prob_ratio}
\end{equation}

In the case that the proposal and target copula densities are assumed to be the same equation \ref{eq:imp_prob_ratio} can be rewritten as \ref{eq:imp_prob_ratio2}.

\begin{equation}
\omega_i = \frac{f_1(x_i) f_2(x_i)}{\tilde f_1(x_i) \tilde f_2(x_i)}
\label{eq:imp_prob_ratio2}
\end{equation}

In this work the target and proposal copula are taken to be the same function.

\subsection{Propagating CRUD Through Time}

\begin{itemize}
        \item (\checkmark) Elucidate why we need a spatial remapping scheme.
        \item (\checkmark) Detail how this mapping is produced.
\end{itemize}

In the construction of the hi2lo method an important assumption is made about the location of hot and cold spots on the rod surface as a function of time.   The presence of hot and cold spots downstream spacer grids and the location on the rod surface these spots occupy are assumed to be principally governed by the geometry of the mixing vanes and geometric layout of the fuel and guide tubes.  The influence of flow rate and core power on the relative location of the hot spots on the rod surface are assumed to be second order effects and are not explicitly captured by the hi2lo methodology at present.

\subsection{Hot Spot Stationarity}

This assumption leads to the notion of hot spot stationarity in time since the geometry of the core does not change throughout a cycle.  In the context of the statistically based hi2lo approach to achieve a stable location of hot and cold spots one establish a mapping from sample space to a location on the rod surface.  To accomplish this, the order statistics of the joint temperate and turbulent kinetic energy distribution in each CTF patch are computed and the condition that the highest order statistic always falls on the same location within a given CTF patch is enforced.

Order statistics are well defined for a single dimensional random variable but in higher dimensions a consistent ordering is not possible.  Therefore, as shown in equation \ref{eq:weighting}, a weighting procedure is used to reduce the random vector of T, TKE, and $q''$ samples on any given CTF patch into a univariate random variable denoted by $\mathbf{m}=\{m_0, m_1, ... m_N\}$.

\begin{equation}
    m_i = w_T \left( \frac{T_i - T_{min}}{T_{max} - T_{min}} \right) + w_k \left( \frac{k_i - k_{min}}{k_{max} - k_{min}} \right) +  w_q'' \left( \frac{q^{''}_i - q^{''}_{min}}{q_{max} - q_{min}} \right)
\label{eq:weighting}
\end{equation}
Where the sample remapping coefficients $\mathbf w$ are set at runtime by the hi2lo model user and sum to 1:
\begin{equation}
w_T + w_k + w_{q''} = 1
\end{equation}
Next the order statistics of $\mathbf m$ are computed such that $\mathbf m' = \{ m_{(0)} < m_{(1)}< ... m_{(N)} \}$.  As shown in figure \ref{fig:samplemapping}, the ordered samples are then emplaced on the CTF patch in an organized manner.  The sample space is denoted by $\Omega$ and a specific temperature, TKE, and boundary heat flux sample is denoted by $\mathcal F_i$ as a single dot residing inside the sample space.  The path taken on the patch surface is user controllable and is taken to be a simple serpentine left-to-right pattern in this work.
Typical values for the remapping coefficients are $w_T=0.6, w_k=0.4, w_{q''}=0$.  With this setting, relatively high temperate and low TKE samples are likely to remain in the same location on the rod surface over multiple re-sampling events.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{../proposal/slides/seminar_slides/figs/sample_mapping}
    \caption{Sample remapping.}
    \label{fig:samplemapping}
\end{figure}

Note that since the crud simulation package utilized in this work is one-dimensional the pattern chosen for sample emplacement has no influence on the integrated crud result over a patch.  This would not be the case if the crud simulation package modeled a fully 3D crud layer as the state of the neighboring crud nodes in that case would matter.  Interestingly, since the hi2lo model user  can specify the sample remapping pattern at run time, a physically realistic pattern could be prescribed on the rod surface - even prescribed as a function of local core conditions leading to a hybrid strategy between the current pure statistical hi2lo procedure and the spatial remapping procedure implemented by Salko et. al.

\subsection{Time Stepping Scheme}

In figure \ref{fig:stepscheme}, the VERA state-point time index is denoted by $t$. Dotted arrows represent sampling from a probability distribution.  $\mathcal R$ is the spatial reordering map from sample space to a position on the rod surface. $\mathcal G$ represents the crud generation function provided by the crud simulation package which takes thermal hydraulic boundary conditions as input in addition to the previous crud state and produces a new crud state.  Re-sampling events are performed at time intervals $\Delta t_s$.  The re-sample index is denoted by $n$.  At each re-sampling event temperature, TKE and boundary heat flux samples are drawn from the reconstructed joint probability $\hat H^t$.  Assume that the sample time step is constant and equal to $\Delta t_s$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/step_scheme}
    \caption{Multi-state point time stepping overview.}
    \label{fig:stepscheme}
\end{figure}

Since the resampling time step size is constant the sample time can be computed by multiplying the re-sampling event index by the constant resampling step size:
\begin{equation}
t_{s_n} = n\Delta t_s
\end{equation}

After applying the constant re-sample step size assumption the time averaged sample weights can be updated at each re-sampling step according to equation \ref{eq:time_sample_weights2}.
The ratio of the target density and the sampling distribution density is denoted by $\omega$ and is given in equation \ref{eq:imp_prob_ratio}.  

\begin{equation}
\bar \omega_{s_n} = \left( \frac{n \Delta t_s}{\sum_i^n \Delta t_{s_i}} \right) \bar \omega_{n-1} + \left( \frac{\Delta t_s}{\sum_i^n \Delta t_{s_i}} \right) \omega_n
\label{eq:time_sample_weights2}
\end{equation}

\subsection{Smearing Over Azimuth}

\begin{itemize}
        \item ($\cdot$) Smear the CFD data and CTF results over all azimuthal angles.  This will increase the number of CFD samples used for the hi2lo mapping construction by a factor of four, and therefore decrease the uncertainty in the sample quantiles by a factor of 2.  See section on the theoretical distribution of sample quantiles (the sample quantile distributions follow 1/sqrt(N) behavior).
        \item ($\cdot$) Since the primary goal of this method is to predict CIPS, it is possible neglect azimuthal variation entirely and focus solely on the axial variation.  In this case, for any given pin, each CTF face at a fixed axial level will all get the same crud thickness and crud boron from the hi2lo model.
\end{itemize}
