%! TEX root = ../dissertation_gurecky.tex

\section{Model Approach}

A fundamental difference between the CFD and CTF computations is the average size of the mesh cells.  In the azimuthal coordinate, CTF decomposes a single rod surface into four patches.  An example top down view of typical CFD and CTF meshes for a single pin are given in figure \ref{fig:cfd_ctf_mesh}.  Though both codes employ a finite volume spatial discretization CFD can resolve the flow at much smaller length scales.  Additionally, each code employs a different set of closure models to the underlying set of coupled energy, mass, and momentum balances.  In practice these differences can lead to large discrepancies in boiling, turbulent mixing, and rod surface temperature predictions between the two codes.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=10cm]{../proposal/images/cfd_ctf_mesh.png}
    \caption{Top-down view of typical subchannel and CFD meshes for a single pin \cite{salko12}.}
    \label{fig:cfd_ctf_mesh}
\end{figure}

Shown in figure \ref{fig:model_overview}, on a given CTF rod surface patch, a single point estimates for the surface temperature, TKE, and heat flux are predicted.  The predicted CTF quantities are an estimate for the average thermal hydraulic conditions over that coarse patch.   Consequentially, CTF CRUD predictions may significantly deviate from reality.  Since CRUD growth is highly sensitive to the presence of subcooled boiling on the rod surface; if CTF predicts a rod surface temperature less than the saturation point very little or no CRUD will form - when in reality, a small portion of that rod surface could exist above the saturation point and thus harbor CRUD.  Small localized mistakes in CRUD predictions compound throughout the core, leading to poor CIPS estimates. 

In the figure $f$ denotes a probability density function whos value can be interpreted as fractional area of the rod surface.  
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=12cm]{../proposal/images/model_relations.png}
    \caption{On a single coarse CTF patch: Differences in CRUD prediction between CFD and CTF models.}
    \label{fig:model_overview}
\end{figure}

\begin{itemize}

    \item CTF estimates mean TH conditions everywhere in the core at a low spatial resolution.  The surrogate provides higher order moments about the mean.
    \begin{equation}
    \mathbf S(\mathbf z) = \underbrace{ \bm \mu(\mathbf{z})}_\text{CTF} +
    \underbrace{\varepsilon({\theta (\bm p(\mathbf z))})}_\text{CFD Informed} + \bm b(\mathbf{z})
    \end{equation}
    \begin{itemize}
        \item $\mathbf S$ is a three component vector field representing the cladding surface temperature, turbulent kinetic energy and boundary heat flux.
        \item $\mathbf z$ are spatial coordinates. $\mathbf p$ are a set of auxiliary predictors.  Auxiliary predictors are covariates that describe local core conditions and may be geometric or thermal hydraulic in nature.
        \item $\varepsilon$ is a random three-component vector field with components: $\{\varepsilon_T, \varepsilon_k, \varepsilon_{q''}\}$.
        \item $\varepsilon({\theta (\bm p(\mathbf z))})$ is a CFD informed model with $\theta$ representing free model parameters which must be fit to CFD data.
        \item $\bm b$ is bias ($\bm \mu_{CTF} - \bm \mu_{CFD}$)
        \item Field averages, $\bm \mu$, are piecewise constant over each CTF patch
        \item Note this is a additive model where Salko constructed a multiplicative model.
    \end{itemize}
\end{itemize}

Consider the case where the CFD results are normally distributed about the CTF results such that $\varepsilon \sim \mathcal N(0, \mathbf \theta(\mathbf p))$, where $\mathbf \theta(\mathbf p) = \bm \Sigma(\mathbf p)$ is a covariance matrix that depends on local core conditions.

Shifting this distribution by a constant vector $\bm c=\bm b + \bm \mu_{ctf}$, results in a new distribution denoted:
\begin{align}
    \left. h \right|_{\bm p} & = \mathcal N(\bm c, \bm \Sigma(\mathbf p)) \nonumber \\
    & = \left.
        \mathcal N \left(
        \begin{pmatrix}
            c_T \\
            c_k \\
            c_{q''}
        \end{pmatrix}
    ,
        \begin{pmatrix}
            \sigma_{T} \sigma_{T} & \sigma_{T} \sigma_{k} & \sigma_{T} \sigma_{q''} \\
            \sigma_{k} \sigma_{T} & \sigma_{k} \sigma_{k} & \sigma_{k} \sigma_{q''} \\
            \sigma_{q''} \sigma_{T} & \sigma_{q''} \sigma_{k} & \sigma_{q''} \sigma_{q''}
        \end{pmatrix}
    \right)
    \right|_{\mathbf p}
\end{align}


The goal is to estimate the expected crud on each CTF patch given by equation \ref{eq:expected_crud}.

\begin{eqnarray}
        total\ crud\ [grams] = A \mu_g\ = A \E[g(\mathbf X|g_o, \mathbf I, \delta t)] \nonumber \\
        = A \iiint g(\mathbf X|g_o, \mathbf I, \delta t) h(\mathbf X|\theta) d \mathbf X
        \label{eq:expected_crud}
\end{eqnarray}
let $\mathbf X= \{T, k, q''\}$ denote a random vector of temperature, TKE, and BHF. $\mathbf I$ represents additional known crud parameters, $g_o$ is the crud state at the start of the time step and $\theta$ are distribution parameters.  The goal is to predict what $h(\cdot)$ is in every CTF face.  The CRUD model, $g(\cdot)$, is common to all ctf faces.

To compute the total crud/boron in each CTF face:
\\

\begin{algorithm}[H]
\KwData{(1) Pre-process training set $\theta(\mathbf p)$.  (2) Train model $M(\mathbf p ; \gamma)$:  $ argmin_\gamma ||M(\mathbf p; \gamma) - \theta(\mathbf p)|| $}

\For{CTF face, $i$}{
Evaluate ML model $\hat \theta_i \leftarrow M(\mathbf p_{i})$ \;
Reconstruct $\hat h_i(x|\hat \theta_i)$ \;
Draw samples $\mathbf X \sim \hat h_i$ \;
Evaluate \ref{eq:expected_crud} via Monte Carlo approximation \;
}
\end{algorithm}
Where $\gamma$ are free parameters in the ML model.
\bigskip

In addition to improving the expected value prediction of CRUD on each CTF patch vs the CTF standalone case, the model provides the capability to estimate the likelihood of extreme value events (i.e. $\mathcal P_f \propto Pr(g(x) > g^*)$, where $g^*$ is some critical crud thickness and $\mathcal P_f$ is a cladding failure probability) on the rod surface.  This would be impossible to quantify with CTF/MAMBA alone.

A significant challenge is computing an estimate for $Var(\mathcal P_f) = \E[(\mathcal P_f - \E(\mathcal P_f))^2]$.


\section{Construction of the Hi2lo Map}

\subsection{Capturing Dependence Between Random Variables}
\begin{itemize}
        \item (\checkmark) Sklar's theorem.
        \item (\checkmark) Assumptions:
        \begin{itemize}
                \item Capture Temperature and TKE dependence structure.
                \item Assume 1) Temperature is uncorrelated with BHF.  2) TKE is uncorrelated with BHF.
                \item Justify 1) showing actual CFD data. 2) Relative variations in BHF are very small over a CTF face $(+/- 5\%)$.  3) Sensitivity of CRUD to BHF is small relative to sensitivity of CRUD to surface temperature.
        \end{itemize}
        \item (\checkmark) Show fallout of these assumptions on hi2lo model.
        \begin{equation}
                h(T, k, q'') = f_T f_k, f_{q''} \prod_c c_i(u_i, v_i)
        \end{equation}
        Where each bivariate copula model in $\prod c_i $ is known as a pair copula construction.

        Applying the aforementioned assumptions results in: $c(u_T, u_{q''}) = 1$ and $c(u_{k}, u_{q''}) = 1$. The simplified joint density is given by:
        \begin{equation}
                h(T, k, q'') \approx  f_T f_k, f_{q''} c(u_{T}, v_{k})  \cdot 1 \cdot 1
    \end{equation}
\end{itemize}


\subsection{Sample Quantiles}

\subsubsection{Non Parametric Representations of Univariate Distributions}

\begin{itemize}
    \item (\checkmark-) Introduce non-parametric representation of univariate distributions through sample quantiles.
    \item ($\cdot$) Explain typical alternatives and why they are not suitable:
        \begin{itemize}
            \item Compute sample moments.  Use method of moments to fit a model to sample moments.
            \item Compute sample cumulants.  Use sample cumulants to build an Edgeworth series.
            \item Do cumulants or traditional moments behave in a predictable manner as a function of local core conditions?  Do the quantiles behave in a predictable manner?
        \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{../proposal/slides/seminar_slides/figs/margins_cdf_2}
    \caption[CDF from quantiles.]{Piecewise linear CDF interpolated from a set of quantiles.}
    \label{fig:marginscdf2}
\end{figure}


The $\tau^{th}$ quantile is $q_\tau = F^{-1}(\tau); $ where $\ F(t)=P[T \leq t]$.
$\tau \in [0, 1]$

The quantile loss function is given by equation \ref{eq:qloss_fn}.
\begin{equation}
\rho_\tau( u) = \mathbf u \cdot (\tau - \mathbb{I}_{( u < 0)})
\label{eq:qloss_fn}
\end{equation}
Where $\mathbb{I}$ is the indicator function which returns 1 if the argument is true and 0 otherwise.
In order to estimate a sample quantile given the empirical CDF $F$, minimize: $\E[\rho_\tau(T - q_\tau)]$ where $T$ is a random variable distributed according to $F$.
\begin{equation}
            \left.\begin{aligned}
            \hat q_{\tau_i} &= argmin_{q} \E[\rho(u)];\ \  u = T - q  \\
            \approx & argmin_q  \frac{1}{N} \sum_i^N \rho(u_i); \ u_i = t_i - q \\
            \approx & argmin_q \left[ (1-\tau) \sum_{y \leq q}( t_i - q ) - \tau \sum_{y > q} (t_i - q) \right]
            \end{aligned}\right.
\end{equation}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{../proposal/slides/seminar_slides/figs/q_loss}
    \caption[Quantile loss function.]{Quantile loss function.}
    \label{fig:qloss}
\end{figure}



\begin{itemize}
        \item (\checkmark) Compute sample quantiles.
        \item (\checkmark) Building a cumulative density function from sample quantiles.
        \begin{itemize}
           \item Piecewise linear CDF leads to histogram PDF.
           \item PCHIP spline CDF preserves monotonicity of CDF and results in a smooth PDF. \cite{Fritsch80}
        \end{itemize}
        \item (\checkmark) Sampling from a known CDF via inverse transform sampling.
        \item (\checkmark-) Show that the order statistics are distributed according to a Gaussian distribution.
        \item (\checkmark-) Show that sample quantiles also follow a Gaussian distribution.  This directly follows from the distribution of the order statistics.
            The sample quantiles are distributed normally according to equation \ref{eq:theory_qdist_1}.
        \begin{eqnarray}
        q_p &\sim \mathcal N \left( F_T^{-1}(p), \sigma^2_{q_p} \right) \\
        \sigma^2_{q_p} &= \frac{p(1 - p)}{n[f_T(F_T^{-1}(p))]^2}
        \label{eq:theory_qdist_1}
        \end{eqnarray}
        


\begin{figure}[]%
    \centering
    \subfloat[Emperical quantiles for a normal distrubtion. 500 trials shown. $0.1, 0.5, 0.9$ quantiles denoted by dashed verticle lines.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual} }}%
    \qquad
        \subfloat[Emperical vs theoredical $0.1$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual_conditional_0_1} }}%
    \qquad
        \subfloat[Emperical vs theoredical $0.5$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual_conditional_0_5} }}%
    \qquad
        \subfloat[Emperical vs theoredical $0.9$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual_conditional_0_9} }}%
    \qquad
    \caption[Distribution of quantiles for a normally distributed RV.]{Distribution of quantiles for a normally distributed RV: $X \sim \mathcal{N}(0, 1)$.  Theoretical quantile standard deviation given by equation \ref{eq:theory_qdist_1}.}%
    \label{fig:normal_q_theory}%
\end{figure}

\begin{figure}[H]%
    \centering
    \subfloat[Emperical quantiles for a beta distrubtion. 500 trials shown. $0.1, 0.5, 0.9$ quantiles denoted by dashed verticle lines.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual} }}%
    \qquad
    \subfloat[Emperical vs theoredical $0.1$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual_conditional_0_1} }}%
    \qquad
    \subfloat[Emperical vs theoredical $0.5$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual_conditional_0_5} }}%
    \qquad
    \subfloat[Emperical vs theoredical $0.9$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual_conditional_0_9} }}%
    \qquad
    \caption[Distribution of quantiles for a beta distributed RV.]{Distribution of quantiles for a beta distributed RV: $X \sim \beta(1, 2)$}%
    \label{fig:ctf_twall_aug}%
\end{figure}


        \item ($\cdot$) Show the impact of the uncertainty in where the sample quantiles fall given CFD data on crud growth.  The reconstructed CDFs which comprise the hi2lo mapping are essentially fuzzy which means the temperature, TKE, and BHF distributions are artificially smeared out (artificially high densities in the tails).
\end{itemize}

\begin{itemize}
    \item (\checkmark-) The sampled temperatures may be tallied over each CTF face to estimate the fractional area that exceeds some threshold temperature.
    The probability of exceeding a threshold temperature is shown in equation \ref{eq:pr_thresh}.
    
    \begin{equation}
    p_e = Pr(T > T^*) = 1 - \int_0^{T^*} f_T dT
    \label{eq:pr_thresh}
    \end{equation}
    
    Let $q_p = F_T^{-1}(1 - p_e)$
    denote the quantile associated with the threshold probability, $p_e$.
    $F_T^{-1}$ is the inverse CDF function and $f_T$ is the probability density function of temperature on the patch.
    
    The sample quantile corresponding to $p_e$ is distributed according to:

   \begin{eqnarray}
    q_p &\sim \mathcal N \left( F_T^{-1}(p), \sigma^2_{q_p} \right) \\
    \sigma^2_{q_p} &= \frac{p(1 - p)}{n[f_T(F_T^{-1}(p))]^2}
    \end{eqnarray}
    
    The variance of upper tail probability mass estimate can be found by standard propagation of uncertainty principles:
   \begin{equation}
    \sigma_p^2 = \left(\frac{\partial p_e}{\partial q_p} \right)^2 \cdot \sigma_{q_p}^2 +\  HOT.
    \end{equation}
    
    Where
   \begin{eqnarray}
    \frac{\partial p_e}{\partial q_p} &= \frac{\partial}{\partial q_p} \left( 1 - \int_0^{q_p} f_T dT \right) \nonumber \\
    &= \frac{\partial}{\partial q_p} \left( -F_T(q_p) + F_T(0) \right) \nonumber \\
    &= -f_T(q_p)
    \end{eqnarray}
    
    This dictates that estimates of extreme upper tail integrals carry large relative uncertainties.
    \item The approximate large sample distribution for $p_e$ can be obtained by a change of variables, since $p_e$ and $q_p$ are related by:
    \begin{equation}
    q_p = F_T^{-1}(1 - p_e)
    \end{equation}
    
    Where $F_T^{-1}$ is the inverse CDF.  $p_e$ is distributed according to:
    
    \begin{equation}
    f_{p_e} = f_{q_p} \left( F_T^{-1}(1-p_e), \sigma_{q_p}(p_e) \right) \cdot \left|{ \frac{\partial}{\partial p_e} F^{-1}(1-p_e) } \right|
    \end{equation}
    
\end{itemize}


\subsection{Introduction Gradient Boosted Regression Trees}
\begin{itemize}
    \item (\checkmark-) Explain a classification and regression tree (CART).  (Move to appendix?)
    \item (\checkmark-) Explain gradient boosting.  (Move to appendix?)
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{../proposal/slides/seminar_slides/figs/cart}
    \caption[Regression tree stump.]{Regression tree stump comparing a fit of depth 1 and 2.}
    \label{fig:cart}
\end{figure}

The generalized gradient boosting algorithm was developed by Friedman (1999).

\begin{algorithm}[H]
    \KwData{ (1) Training set $\{(p_i, y_i)\}_{i=1}^n$. (2) Differentiable loss function $L(y, F(p))$. (3) Number of iterations ${{M}}$.
        Initialize model with a constant value:
        $F_0(p) = \underset{\gamma}{\arg\min} \sum_{i=1}^n L(y_i, \gamma).$}
    
    \For{${{m}} = 1$ to ${{M}}$}{
        Compute the pseudo-residuals:
        
        \For{$i=1,\ldots,n $}{
            $r_{im} = -\frac{\partial L(y_i, F_{m-1}(p_i))}{\partial F_{m-1}(p_i)}$
        }
        
        Fit a weak learner $h_m(p)$ to pseudo-residuals, $r_{m}$: Training data set is $\{(p_i, r_{im})\}_{i=1}^n$ \;
        
        Compute multiplier $\gamma_m$ :
        $\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, F_{m-1}(p_i) + \gamma h_m(p_i)\right)$\;
        Update the model:
        $F_m(p) = F_{m-1}(p) + \nu \gamma_m h_m(p).$
    }
    Output $F_M(p).$
\end{algorithm}
Where $\nu$ is a tunable constant in $[0, 1]$ called the learning rate.


\subsection{Monte Carlo CRUD Estimation}

\begin{itemize}
        \item (\checkmark-) Show importance sampling scheme to estimate \ref{eq:expected_crud}.  (Move to Appendix?)
        \begin{equation}
        \E(g(x)) \approx \frac{1}{N} \sum_i^N g(x_i) \frac{h(x_i)}{\tilde h(x_i)}, \ x \sim \tilde{h}
        \end{equation}
        \item (\checkmark-) Show examples of $g(x)$, $h(x)$ and $\tilde h(x)$ on a single CTF face.
        \item (\xmark) \sout{Find optimal proposal density distribution, $\tilde{h^*}$.}
\end{itemize}



\subsection{Propagating CRUD Through Time}

\begin{itemize}
        \item (\checkmark) Elucidate why we need a spatial remapping scheme.
        \item (\checkmark) Detail how this mapping is produced.
\end{itemize}

In the construction of the hi2lo method an important assumption is made about the location of hot and cold spots on the rod surface as a function of time.   The presence of hot and cold spots downstream spacer grids and the location on the rod surface these spots occupy are assumed to be principally governed by the geometry of the mixing vanes and geometric layout of the fuel and guide tubes.  The influence of flow rate and core power on the relative location of the hot spots on the rod surface are assumed to be second order effects and are not explicitly captured by the hi2lo methodology at present.

\subsubsection{Hot Spot Stationarity}

This assumption leads to the notion of hot spot stationarity in time since the geometry of the core does not change throughout a cycle.  In the context of the statistically based hi2lo approach to achieve a stable location of hot and cold spots one must structure the samples.  To accomplish this, one must compute the order statistics of the joint temperate and turbulent kinetic energy distribution in each CTF patch and ensure that the highest order statistic always falls on the same location within a given CTF patch.

Order statistics are well defined for a single dimensional random variable but in higher dimensions, a consistent ordering is not possible.  Therefore, as shown in equation \ref{eq:weighting}, a weighting procedure is used to reduce the trivariate random vector of T, TKE, and $q''$ on any given CTF patch into a univariate random variable denoted by $\mathbf{m}=\{m_0, m_1, ... m_N\}$.

\begin{equation}
m_i = w_T \left( \frac{T_i}{T_{max} - T_{min}} \right) + w_k \left( \frac{k_i}{k_{max} - k_{min}} \right) +  w_q'' \left( \frac{q^{''}_i}{q_{max} - q_{min}} \right)
\label{eq:weighting}
\end{equation}
Where the sample remapping coefficients $\mathbf w$ are set at runtime by the hi2lo model user and sum to 1:
\begin{equation}
w_T + w_k + w_{q''} = 1
\end{equation}
Next the order statistics of $\mathbf m$ are computed such that $\mathbf m' = \{ m_{(0)} < m_{(1)}< ... m_{(N)} \}$.  As shown in figure \ref{fig:samplemapping}, the ordered samples are then emplaced on the CTF patch in an organized manner.  The path taken on the patch surface is user controllable and is taken to be a simple serpentine left-to-right pattern in this work.
Typical values for the remapping coefficients are $w_T=0.6, w_k=0.4, w_{q''}=0$.  With this setting, relatively high temperate and low TKE samples are likely to remain in the same location on the rod surface over multiple resampling events.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{../proposal/slides/seminar_slides/figs/sample_mapping}
    \caption{}
    \label{fig:samplemapping}
\end{figure}

Note that since the crud simulation package utilized in this work is one-dimensional the pattern chosen for sample emplacement has no influence on the integrated crud result over a patch.  This would not be the case if the crud simulation package modeled a fully 3D crud layer as the state of the neighboring crud nodes in that case would matter.  Interestingly, since the hi2lo model user  can specify the sample remapping pattern at run time, a physically realistic pattern could be prescribed on the rod surface - even prescribed as a function of local core conditions leading to a hybrid strategy between the current pure statistical hi2lo procedure and the spatial remapping procedure implemented by Salko et. al.

\subsection{Smearing Over Azimuth}

\begin{itemize}
        \item ($\cdot$) Smear the CFD data and CTF results over all azimuthal angles.  This will increase the number of CFD samples used for the hi2lo mapping construction by a factor of four, and therefore decrease the uncertainty in the sample quantiles by a factor of 2.  See section on the theoretical distribution of sample quantiles (the sample quantile distributions follow 1/sqrt(N) behavior).
        \item ($\cdot$) Since the primary goal of this method is to predict CIPS, it is possible neglect azimuthal variation entirely and focus solely on the axial variation.  In this case, for any given pin, each CTF face at a fixed axial level will all get the same crud thickness and crud boron from the hi2lo model.
\end{itemize}
