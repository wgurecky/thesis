%! TEX root = ../dissertation_gurecky.tex

\section{Model Approach}

A fundamental difference between the CFD and CTF computations is the average size of the mesh cells.  In the azimuthal coordinate, CTF decomposes a single rod surface into four patches.  An example top down view of typical CFD and CTF meshes for a single pin are given in figure \ref{fig:cfd_ctf_mesh}.  Though both codes employ a finite volume spatial discretization CFD can resolve the flow at much smaller length scales.  Additionally, each code employs a different set of closure models to the underlying set of coupled energy, mass, and momentum balances.  In practice these differences can lead to large discrepancies in boiling, turbulent mixing, and rod surface temperature predictions between the two codes.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=10cm]{../proposal/images/cfd_ctf_mesh.png}
    \caption{Top-down view of typical subchannel and CFD meshes for a single pin \cite{salko12}.}
    \label{fig:cfd_ctf_mesh}
\end{figure}

Shown in figure \ref{fig:model_overview}, on a given CTF rod surface patch, a single point estimates for the surface temperature, TKE, and heat flux are predicted.  The predicted CTF quantities are an estimate for the average thermal hydraulic conditions over that coarse patch.   Consequentially, CTF CRUD predictions may significantly deviate from reality.  Since CRUD growth is highly sensitive to the presence of subcooled boiling on the rod surface; if CTF predicts a rod surface temperature less than the saturation point very little or no CRUD will form - when in reality, a small portion of that rod surface could exist above the saturation point and thus harbor CRUD.  Small localized mistakes in CRUD predictions compound throughout the core, leading to poor CIPS estimates. 

In figure \ref{fig:model_overview}, $f$ denotes a probability density function.  Integration of this density function can be interpreted as computing a fractional area of the rod surface that exists within the specified integration limits.  
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=12cm]{../proposal/images/model_relations.png}
    \caption{On a single coarse CTF patch: Differences in CRUD prediction between CFD and CTF models.}
    \label{fig:model_overview}
\end{figure}

CTF estimates mean TH conditions everywhere in the core at a low spatial resolution.  The CFD informed model provides higher order moments about the mean.
\begin{equation}
   \mathbf S(\mathbf p, \mathbf z) = \underbrace{ \bm \mu(\mathbf p, \mathbf{z})}_\text{CTF} +
   \underbrace{\varepsilon({\theta (\bm p, \mathbf z)})}_\text{CFD Informed} + \bm b(\mathbf p, \mathbf{z})
   \label{eq:hi2lo_overview}
\end{equation}

\begin{itemize}
        \item $\mathbf S$ is a three component vector field representing the cladding surface temperature, turbulent kinetic energy and boundary heat flux.
        \item $\mathbf z$ denotes spatial coordinates and $\mathbf p$ represents a set of auxiliary predictors.  Auxiliary predictors are covariates that describe local core conditions and may be geometric or thermal hydraulic in nature.
        \item $\varepsilon$ is a random three-component vector field with components: $\{\varepsilon_T, \varepsilon_k, \varepsilon_{q''}\}$.
        \item $\varepsilon$ is a CFD informed model with $\theta$ representing free model parameters which must be determined given CFD data.
        \item $\bm b$ is bias between the low and high fidelity models ($\bm \mu_{CTF} - \bm \mu_{CFD}$).  Despite providing identical inlet boundary conditions to both codes bias exists due to algorithmic differences, closure model differences, and meshing differences between the two codes.
        \item Field averages, $\bm \mu$, are piecewise constant over each CTF patch.
        \item Note this is a additive model where Salko constructed a multiplicative model.
\end{itemize}

Consider the hypothetical case where the CFD results are normally distributed about the CTF results such that $\varepsilon \sim \mathcal N(0, \mathbf \theta(\mathbf p, \mathbf z))$, where $\mathbf \theta(\mathbf p, \mathbf z) = \bm \Sigma(\mathbf p, \mathbf z)$ is a covariance matrix that depends on local core conditions.  Shifting this distribution by a constant vector $\bm c=\bm b + \bm \mu_{ctf}$, results in a new distribution denoted by $h$ in equation \ref{eq:norm_noise}.
\begin{align}
    \left. h \right|_{(\bm p, \bm z)} & = \mathcal N(\bm c, \bm \Sigma(\mathbf p, \bm z)) \nonumber \\
    & = \left.
        \mathcal N \left(
        \begin{pmatrix}
            c_T \\
            c_k \\
            c_{q''}
        \end{pmatrix}
    ,
        \begin{pmatrix}
            \sigma_{T} \sigma_{T} & \sigma_{T} \sigma_{k} & \sigma_{T} \sigma_{q''} \\
            \sigma_{k} \sigma_{T} & \sigma_{k} \sigma_{k} & \sigma_{k} \sigma_{q''} \\
            \sigma_{q''} \sigma_{T} & \sigma_{q''} \sigma_{k} & \sigma_{q''} \sigma_{q''}
        \end{pmatrix}
    \right)
    \right|_{(\mathbf p, \mathbf z)}
\label{eq:norm_noise}
\end{align}


The goal is to estimate the expected crud mass $C_m$ on each CTF patch given by equation \ref{eq:expected_crud}.  Let $\mathbf X= \{T, k, q''\}$ denote a random vector of temperature, TKE, and BHF. $\mathbf I$ represents additional known crud parameters, $\mathbf C_o$ is the crud state at the start of the time step and $\theta$ are distribution parameters.    The CRUD model, $\mathcal G(\cdot)$, is common to all CTF faces.  The joint cumulative density's parameters, $\theta$, $(\hat H(\cdot|\theta))$ must be predicted from the available high resolution CFD data in every CTF face.  In the case of an assumed normal distribution model there are nine unknowns:  $\{ \sigma^2_T, \sigma^2_k, \sigma^2_{q''}, \sigma_k \sigma_T, \sigma_{q''}\sigma_T, \sigma_{q''}\sigma_{k}, c_T, c_k, c_{q''} \}$.  In the subsequent sections we will seek to relax this normality assumption of the CFD residuals about the CTF result.

\begin{align}
        C_m &= A \mu_{m} \nonumber \\
        &= A \E[\mathcal G(\mathbf X|\mathbf C_o, \mathbf I, \delta t)] \nonumber \\
        &= A \iiint \mathcal G(\mathbf X|\mathbf C_o, \mathbf I, \delta t) h(\mathbf X|\theta) d \mathbf X
        \label{eq:expected_crud}
\end{align}

A strategy to compute the unknowns of the join distribution on each CTF face is required.  In this work we propose a data driven model, $\mathcal M$, to predict the unknowns provided a suite of pre-computed CFD results are used to train the machine learning model and local thermal hydraulic conditions provided by CTF at runtime are utilized to evaluate the model on each CTF face. Algorithm \ref{algo:basic_crud_algo} is used to compute the total crud mass in each CTF face.

\begin{algorithm}[H]
    \captionsetup{labelfont={sc,bf}, labelsep=newline}
    \caption{Generic hi2lo method for crud prediction.}
    \begin{algorithmic}      
    \STATE \textbf{Initialization}  
    \STATE (1) Pre-process training set.  
    \STATE $\ \ $   (1b) Fit the joint distribution parameters to known CFD data: $\theta(\mathbf p, \mathbf z)$.  
    \STATE $\ \ $   (1c) \textbf{def:}  $\ \theta \leftarrow \mathcal M(\mathbf p, \mathbf z | \gamma)$
    \STATE (2) Train model:  $\hat \gamma =  argmin_\gamma ||\mathcal M(\mathbf p, \mathbf z| \gamma) - \theta(\mathbf p, \mathbf z)|| $
\FOR {CTF face, $j$}
    \STATE Evaluate ML model $\hat \theta_j \leftarrow \mathcal M(\mathbf p_{j}, \mathbf z_j; \hat \gamma)$ \;
    \STATE Reconstruct $\hat H_j(\cdot |\hat \theta_j)$ \;
    \STATE Draw samples $\mathbf X \sim \hat H_j$ \;
    \STATE Evaluate \ref{eq:expected_crud} via Monte Carlo approximation \;
\ENDFOR
    \end{algorithmic}
\label{algo:basic_crud_algo}
\end{algorithm}
Where $\gamma$ are free parameters in the machine learning regression model.  A discussion on the machine learning model follows in section ().  The reconstruction of the joint density function $\hat H$ from copula and univariate quantile functions is discussed in section ().  Monte Carlo and importance sampling are discussed in sections () and () respectively.  A critical question to answer is:  Can a data driven machine learning model adequately predict the parameters of a probability distribution on each CTF face: $Var[(\hat \theta_j \leftarrow \mathcal M(\mathbf p_{j}, \gamma))] < \epsilon$?  There is some uncertainty in the optimal machine learning model parameters.  This propagates to the distribution parameters which ultimately governs the uncertainty of the crud results.  An adequet model in this contex reduces the uncertainty in the final crud estimate to some acceptable level.
\bigskip

In addition to improving the expected value prediction of CRUD on each CTF patch vs the CTF standalone case, the model provides the capability to estimate the likelihood of extreme value events (i.e. $\mathcal P_f \propto Pr(C_t(x) > C_t^*)$, where $C_t^*$ is some critical crud thickness and $\mathcal P_f$ is a cladding failure probability) on the rod surface.  This would be impossible to quantify with CTF/MAMBA alone.  A significant challenge is computing an estimate for $Var(\mathcal P_f) = \E[(\mathcal P_f - \E(\mathcal P_f))^2]$.


\section{Construction of the Hi2lo Map}

\subsection{Capturing Dependence Between Random Variables}
\begin{itemize}
        \item (\checkmark) Sklar's theorem.
        \item (\checkmark) Assumptions:
        \begin{itemize}
                \item Capture Temperature and TKE dependence structure.
                \item Assume 1) Temperature is uncorrelated with BHF.  2) TKE is uncorrelated with BHF.
                \item Justify 1) showing actual CFD data. 2) Relative variations in BHF are very small over a CTF face $(+/- 5\%)$.  3) Sensitivity of CRUD to BHF is small relative to sensitivity of CRUD to surface temperature.
        \end{itemize}
        \item (\checkmark) Show fallout of these assumptions on hi2lo model.
        \begin{equation}
                h(T, k, q'') = f_T f_k, f_{q''} \prod_c c_i(u_i, v_i)
        \end{equation}
        Where each bivariate copula model in $\prod c_i $ is known as a pair copula construction.

        Applying the aforementioned assumptions results in: $c(u_T, u_{q''}) = 1$ and $c(u_{k}, u_{q''}) = 1$. The simplified joint density is given by:
        \begin{equation}
                h(T, k, q'') \approx  f_T f_k, f_{q''} c(u_{T}, v_{k})  \cdot 1 \cdot 1
    \end{equation}
\end{itemize}


\subsection{Sample Quantiles}

\subsubsection{Non Parametric Representations of Univariate Distributions}

\begin{itemize}
    \item (\checkmark-) Introduce non-parametric representation of univariate distributions through sample quantiles.
    \item ($\cdot$) Explain typical alternatives and why they are not suitable:
        \begin{itemize}
            \item Compute sample moments.  Use method of moments to fit a model to sample moments.
            \item Compute sample cumulants.  Use sample cumulants to build an Edgeworth series.
            \item Do cumulants or traditional moments behave in a predictable manner as a function of local core conditions?  Do the quantiles behave in a predictable manner?
        \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{../proposal/slides/seminar_slides/figs/margins_cdf_2}
    \caption[CDF from quantiles.]{Piecewise linear CDF interpolated from a set of quantiles.}
    \label{fig:marginscdf2}
\end{figure}


The $\tau^{th}$ quantile is $q_\tau = F^{-1}(\tau); $ where $\ F(t)=P[T \leq t]$.
$\tau \in [0, 1]$

The quantile loss function is given by equation \ref{eq:qloss_fn}.
\begin{equation}
\rho_\tau( u) = \mathbf u \cdot (\tau - \mathbb{I}_{( u < 0)})
\label{eq:qloss_fn}
\end{equation}
Where $\mathbb{I}$ is the indicator function which returns 1 if the argument is true and 0 otherwise.
In order to estimate a sample quantile given the empirical CDF $F$, minimize: $\E[\rho_\tau(T - q_\tau)]$ where $T$ is a random variable distributed according to $F$.
\begin{equation}
            \left.\begin{aligned}
            \hat q_{\tau_i} &= argmin_{q} \E[\rho(u)];\ \  u = T - q  \\
            \approx & argmin_q  \frac{1}{N} \sum_i^N \rho(u_i); \ u_i = t_i - q \\
            \approx & argmin_q \left[ (1-\tau) \sum_{y \leq q}( t_i - q ) - \tau \sum_{y > q} (t_i - q) \right]
            \end{aligned}\right.
\end{equation}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{../proposal/slides/seminar_slides/figs/q_loss}
    \caption[Quantile loss function.]{Quantile loss function.}
    \label{fig:qloss}
\end{figure}



\begin{itemize}
        \item (\checkmark) Compute sample quantiles.
        \item (\checkmark) Building a cumulative density function from sample quantiles.
        \begin{itemize}
           \item Piecewise linear CDF leads to histogram PDF.
           \item PCHIP spline CDF preserves monotonicity of CDF and results in a smooth PDF. \cite{Fritsch80}
        \end{itemize}
        \item (\checkmark) Sampling from a known CDF via inverse transform sampling.
        \item (\checkmark-) Show that the order statistics are distributed according to a Gaussian distribution.
        \item (\checkmark-) Show that sample quantiles also follow a Gaussian distribution.  This directly follows from the distribution of the order statistics.
            The sample quantiles are distributed normally according to equation \ref{eq:theory_qdist_1}.
        \begin{eqnarray}
        q_p &\sim \mathcal N \left( F_T^{-1}(p), \sigma^2_{q_p} \right) \\
        \sigma^2_{q_p} &= \frac{p(1 - p)}{n[f_T(F_T^{-1}(p))]^2}
        \label{eq:theory_qdist_1}
        \end{eqnarray}
        


\begin{figure}[]%
    \centering
    \subfloat[Emperical quantiles for a normal distrubtion. 500 trials shown. $0.1, 0.5, 0.9$ quantiles denoted by dashed verticle lines.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual} }}%
    \qquad
        \subfloat[Emperical vs theoredical $0.1$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual_conditional_0_1} }}%
    \qquad
        \subfloat[Emperical vs theoredical $0.5$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual_conditional_0_5} }}%
    \qquad
        \subfloat[Emperical vs theoredical $0.9$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_gauss_residual_conditional_0_9} }}%
    \qquad
    \caption[Distribution of quantiles for a normally distributed RV.]{Distribution of quantiles for a normally distributed RV: $X \sim \mathcal{N}(0, 1)$.  Theoretical quantile standard deviation given by equation \ref{eq:theory_qdist_1}.}%
    \label{fig:normal_q_theory}%
\end{figure}

\begin{figure}[H]%
    \centering
    \subfloat[Emperical quantiles for a beta distrubtion. 500 trials shown. $0.1, 0.5, 0.9$ quantiles denoted by dashed verticle lines.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual} }}%
    \qquad
    \subfloat[Emperical vs theoredical $0.1$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual_conditional_0_1} }}%
    \qquad
    \subfloat[Emperical vs theoredical $0.5$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual_conditional_0_5} }}%
    \qquad
    \subfloat[Emperical vs theoredical $0.9$ quantile distribution.]{{\includegraphics[width=0.45\linewidth]{figs/quantile_theory/q_beta_residual_conditional_0_9} }}%
    \qquad
    \caption[Distribution of quantiles for a beta distributed RV.]{Distribution of quantiles for a beta distributed RV: $X \sim \beta(1, 2)$}%
    \label{fig:ctf_twall_aug}%
\end{figure}


        \item ($\cdot$) Show the impact of the uncertainty in where the sample quantiles fall given CFD data on crud growth.  The reconstructed CDFs which comprise the hi2lo mapping are essentially fuzzy which means the temperature, TKE, and BHF distributions are artificially smeared out (artificially high densities in the tails).
\end{itemize}

\begin{itemize}
    \item (\checkmark-) The sampled temperatures may be tallied over each CTF face to estimate the fractional area that exceeds some threshold temperature.
    The probability of exceeding a threshold temperature is shown in equation \ref{eq:pr_thresh}.
    
    \begin{equation}
    p_e = Pr(T > T^*) = 1 - \int_0^{T^*} f_T dT
    \label{eq:pr_thresh}
    \end{equation}
    
    Let $q_p = F_T^{-1}(1 - p_e)$
    denote the quantile associated with the threshold probability, $p_e$.
    $F_T^{-1}$ is the inverse CDF function and $f_T$ is the probability density function of temperature on the patch.
    
    The sample quantile corresponding to $p_e$ is distributed according to:

   \begin{eqnarray}
    q_p &\sim \mathcal N \left( F_T^{-1}(p), \sigma^2_{q_p} \right) \\
    \sigma^2_{q_p} &= \frac{p(1 - p)}{n[f_T(F_T^{-1}(p))]^2}
    \end{eqnarray}
    
    The variance of upper tail probability mass estimate can be found by standard propagation of uncertainty principles:
   \begin{equation}
    \sigma_p^2 = \left(\frac{\partial p_e}{\partial q_p} \right)^2 \cdot \sigma_{q_p}^2 +\  HOT.
    \end{equation}
    
    Where
   \begin{eqnarray}
    \frac{\partial p_e}{\partial q_p} &= \frac{\partial}{\partial q_p} \left( 1 - \int_0^{q_p} f_T dT \right) \nonumber \\
    &= \frac{\partial}{\partial q_p} \left( -F_T(q_p) + F_T(0) \right) \nonumber \\
    &= -f_T(q_p)
    \end{eqnarray}
    
    This dictates that estimates of extreme upper tail integrals carry large relative uncertainties.
    \item The approximate large sample distribution for $p_e$ can be obtained by a change of variables, since $p_e$ and $q_p$ are related by:
    \begin{equation}
    q_p = F_T^{-1}(1 - p_e)
    \end{equation}
    
    Where $F_T^{-1}$ is the inverse CDF.  $p_e$ is distributed according to:
    
    \begin{equation}
    f_{p_e} = f_{q_p} \left( F_T^{-1}(1-p_e), \sigma_{q_p}(p_e) \right) \cdot \left|{ \frac{\partial}{\partial p_e} F^{-1}(1-p_e) } \right|
    \end{equation}
    
\end{itemize}


\subsection{Introduction Gradient Boosted Regression Trees}
\begin{itemize}
    \item (\checkmark-) Explain a classification and regression tree (CART).  (Move to appendix?)
    \item (\checkmark-) Explain gradient boosting.  (Move to appendix?)
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{../proposal/slides/seminar_slides/figs/cart}
    \caption[Regression tree stump.]{Regression tree stump comparing a fit of depth 1 and 2.}
    \label{fig:cart}
\end{figure}

The generalized gradient boosting algorithm was developed by Friedman (1999).  This work was significant because it re-invisioned previous boosting algorithims such as adaBoost as specific special cases of general gradient boosting with specific loss functions.  Gradient boosting is optimization in a function space.

This hi2lo work employs gradient tree boosting in which simple desscission trees are greedly fit to the gradient of the loss function, or so called "pseudo residuals" since if the loss function is taken to be the L2 loss, the gradient of the loss functions coincides with a residual vector.  In this  weak learner is defined to be a typical classification and regression tree (CART).  Methods for pruning the tree and optimizing the numerical implementation of finding the best splits are left to (refs).

\begin{algorithm}[H]
    \captionsetup{labelfont={sc,bf}, labelsep=newline}
    \caption{Gradient boosting algorithm (ref).}
    \begin{algorithmic}
    \STATE \textbf{Initialization} 
    \STATE (1) Training set $\{(p_i, y_i)\}_{i=1}^n$. 
    \STATE (2) Differentiable loss function $L(y, F(p))$. 
    \STATE (3) Number of iterations ${{M}}$.
    \STATE (4)   Initialize model with a constant value:
        $F_0(p) = \underset{\gamma}{\arg\min} \sum_{i=1}^n L(y_i, \gamma).$
    
    \FOR {${{m}} = 1$ to ${{M}}$}
        \STATE Compute the pseudo-residuals:  
        \FOR {$i=1,\ldots,n $}
            \STATE $r_{im} = -\frac{\partial L(y_i, F_{m-1}(p_i))}{\partial F_{m-1}(p_i)}$
        \ENDFOR
        
        \STATE Fit a weak learner $h_m(p)$ to pseudo-residuals, $r_{m}$: Training data set is $\{(p_i, r_{im})\}_{i=1}^n$ \;
        
        \STATE Compute multiplier $\gamma_m$ :
        $\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, F_{m-1}(p_i) + \gamma h_m(p_i)\right)$\;
        \STATE Update the model:
        $F_m(p) = F_{m-1}(p) + \nu \gamma_m h_m(p).$
    
    \ENDFOR
    \STATE Output $F_M(p).$
    \end{algorithmic}
\end{algorithm}
Where $\nu$ is a tunable constant in $[0, 1]$ called the learning rate.


\subsection{Monte Carlo CRUD Estimation}

\begin{itemize}
        \item (\checkmark-) Show importance sampling scheme to estimate \ref{eq:expected_crud}.  (Move to Appendix?)
        \begin{equation}
        \E(g(x)) \approx \frac{1}{N} \sum_i^N g(x_i) \frac{h(x_i)}{\tilde h(x_i)}, \ x \sim \tilde{h}
        \end{equation}
        \item (\checkmark-) Show examples of $g(x)$, $h(x)$ and $\tilde h(x)$ on a single CTF face.
        \item (\xmark) \sout{Find optimal proposal density distribution, $\tilde{h^*}$.}
\end{itemize}



\subsection{Propagating CRUD Through Time}

\begin{itemize}
        \item (\checkmark) Elucidate why we need a spatial remapping scheme.
        \item (\checkmark) Detail how this mapping is produced.
\end{itemize}

In the construction of the hi2lo method an important assumption is made about the location of hot and cold spots on the rod surface as a function of time.   The presence of hot and cold spots downstream spacer grids and the location on the rod surface these spots occupy are assumed to be principally governed by the geometry of the mixing vanes and geometric layout of the fuel and guide tubes.  The influence of flow rate and core power on the relative location of the hot spots on the rod surface are assumed to be second order effects and are not explicitly captured by the hi2lo methodology at present.

\subsection{Hot Spot Stationarity}

This assumption leads to the notion of hot spot stationarity in time since the geometry of the core does not change throughout a cycle.  In the context of the statistically based hi2lo approach to achieve a stable location of hot and cold spots one establish a mapping from sample space to a location on the rod surface.  To accomplish this, the order statistics of the joint temperate and turbulent kinetic energy distribution in each CTF patch are computed and the condition that the highest order statistic always falls on the same location within a given CTF patch is enforced.

Order statistics are well defined for a single dimensional random variable but in higher dimensions a consistent ordering is not possible.  Therefore, as shown in equation \ref{eq:weighting}, a weighting procedure is used to reduce the trivariate random vector of T, TKE, and $q''$ on any given CTF patch into a univariate random variable denoted by $\mathbf{m}=\{m_0, m_1, ... m_N\}$.

\begin{equation}
m_i = w_T \left( \frac{T_i}{T_{max} - T_{min}} \right) + w_k \left( \frac{k_i}{k_{max} - k_{min}} \right) +  w_q'' \left( \frac{q^{''}_i}{q_{max} - q_{min}} \right)
\label{eq:weighting}
\end{equation}
Where the sample remapping coefficients $\mathbf w$ are set at runtime by the hi2lo model user and sum to 1:
\begin{equation}
w_T + w_k + w_{q''} = 1
\end{equation}
Next the order statistics of $\mathbf m$ are computed such that $\mathbf m' = \{ m_{(0)} < m_{(1)}< ... m_{(N)} \}$.  As shown in figure \ref{fig:samplemapping}, the ordered samples are then emplaced on the CTF patch in an organized manner.  The sample space is denoted by $\Omega$ and a specific temperature, TKE, and boundary heat flux sample is denoted by $\mathcal F_i$ as a single dot residing inside the sample space.  The path taken on the patch surface is user controllable and is taken to be a simple serpentine left-to-right pattern in this work.
Typical values for the remapping coefficients are $w_T=0.6, w_k=0.4, w_{q''}=0$.  With this setting, relatively high temperate and low TKE samples are likely to remain in the same location on the rod surface over multiple resampling events.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{../proposal/slides/seminar_slides/figs/sample_mapping}
    \caption{Sample remapping.}
    \label{fig:samplemapping}
\end{figure}

Note that since the crud simulation package utilized in this work is one-dimensional the pattern chosen for sample emplacement has no influence on the integrated crud result over a patch.  This would not be the case if the crud simulation package modeled a fully 3D crud layer as the state of the neighboring crud nodes in that case would matter.  Interestingly, since the hi2lo model user  can specify the sample remapping pattern at run time, a physically realistic pattern could be prescribed on the rod surface - even prescribed as a function of local core conditions leading to a hybrid strategy between the current pure statistical hi2lo procedure and the spatial remapping procedure implemented by Salko et. al.

\subsection{Time Stepping Scheme}

In figure \ref{fig:stepscheme}, the VERA state-point time index is denoted by $t$. Dotted arrows represent sampling from a probability distribution and solid samples represent a mapping, with $\mathcal R$ being the spatial reordering map from sample space to a position on the rod surface. $\mathcal G$ represents the crud generation function provided by the crud simulation package which takes thermal hydraulic boundary conditions as input in addition to the previous crud state and produces a new crud state.  Resampling events are performed at time intervals $\Delta t_s$.  The resample index is denoted by $n$.  At each resampling event temperature, TKE and boundary heat flux samples are drawn from the reconstructed joint probability $\hat H^t$.  Assume that the sample time step is constant and equal to $\Delta s$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/step_scheme}
    \caption{Multi-state point time stepping overview.}
    \label{fig:stepscheme}
\end{figure}

Since the resampling time step size is constant the sample time denoted by $s$, can be computed by multiplying the resampling event index by the constant resampling step size:
\begin{equation}
s_n = n\Delta t_s
\end{equation}

The time averaged sample weights are updated according to equation \ref{eq:time_sample_weights1}

\begin{equation}
\bar \omega_s = \left( \frac{1}{\sum_i^n \Delta t_{s_i}} \right) \sum_{i=0}^n \omega_i \Delta t_{s_i}
\label{eq:time_sample_weights1}
\end{equation}

And $\omega_i$ is the ratio of the target density and the sampling distribution density as given in equation ().  Applying uniform resampling step size assumption results in equation \ref{eq:time_sample_weights2}.

\begin{equation}
\bar \omega_s = \left( \frac{\Delta t_s}{\sum_i^n \Delta t_{s_i}} \right) \sum_{i=0}^n \omega_i
\label{eq:time_sample_weights2}
\end{equation}

\subsection{Smearing Over Azimuth}

\begin{itemize}
        \item ($\cdot$) Smear the CFD data and CTF results over all azimuthal angles.  This will increase the number of CFD samples used for the hi2lo mapping construction by a factor of four, and therefore decrease the uncertainty in the sample quantiles by a factor of 2.  See section on the theoretical distribution of sample quantiles (the sample quantile distributions follow 1/sqrt(N) behavior).
        \item ($\cdot$) Since the primary goal of this method is to predict CIPS, it is possible neglect azimuthal variation entirely and focus solely on the axial variation.  In this case, for any given pin, each CTF face at a fixed axial level will all get the same crud thickness and crud boron from the hi2lo model.
\end{itemize}
