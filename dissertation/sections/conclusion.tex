%! TEX root = ../dissertation_gurecky.tex

This work joined gradient boosted quantile regression with copula to enable the joint temperature and TKE probability density on each CTF face to be predicted as a function of local core conditions.  This enabled the evaluation of the total crud deposited on each CTF face via Monte Carlo integration.  Additionally, the impact of hot spot stationarity assumptions were investigated.  From this study it was shown that reordering samples within a CTF face at each sub-sampling step was an effective method to preserve hot spot stationarity.  In addition to increasing the sample size, the ability to take many sub-sampling steps per VERA state point was shown to reduce Monte Carlo sampling uncertainties in the time stepping scheme.  Finally, importance sampling was shown to be an effective means to increase sample efficiency.

\section{Discussion}

The application of copula allowed for independent treatment of the temperature, TKE, boundary heat flux rod surface fields and their corresponding dependence structure.  Treatment of the boundary heat flux as an independent random variable allowed further simplification of the copula model.  The decomposition of the joint density into marginal models and a copula allowed for the marginal densities to be reconstructed via multiple quantile regression.  Gradient boosted quantile regressions were employed in this role.

Employing a gradient boosted machine learning model in the role of quantile regression affords a great deal of flexibility in the selection of predictive features used in the model's construction.  A suite of exogenous variables which could be obtained through VERA or CTF results were identified, though, it was recognize that additional geometric pin and grid features could be included into the exogenous variable set in the future.  The boosted models were shown to be robust to discontinuities in the response variables which is a required property in the current application given the sharp jumps seen in the temperature, TKE, and rank correlation coefficient behavior across spacer grids. 

An investigation of the single dimensional crud code's response to varying surface temperature, TKE, and boundary heat flux lead to the development of tailored proposal density distributions for use in importance sampling.   The proposal distributions target the upper tail of the temperature distribution and lower tail of the TKE distribution so that a larger proportion of the available samples are expended on regions of the rods' surface which are more likely to harbor crud.  

The ability to produce a hi2lo mapping in the case of missing CFD data at the desired TH state point was demonstrated through a leave one out cross validation study.  This is a capability any candidate hi2lo tool must posses since not all geometric and thermal hydraulic conditions can be simulated up-front, so inevitably the a hi2lo mapping must be producible in cases where precisely matching CFD data does not exist in the training pool. However, no method was in place to detect if the boosted models were queried outside of their training envelope.  While the present model still produces predictions when extrapolated, the predictions are not credible.  Boosted trees are particularly sensitive to extrapolation since boosted tree models produce piecewise constant predictions.

The overarching strategy sought to to estimate the expected value of the crud mass (and crud boron mass) in each CTF face by first reconstructing the joint density distribution of key surface field quantities on each face and then carrying out the expected value calculation through Monte Carlo integration.  This approach opens up tremendous flexibility in the methods chosen to reconstruct the joint densities - though certain required model characteristics were identified such as robustness to discontinuities.  In future studies, deep learning strategies or stacked machine learning models could be employed in place of the gradient boosted models demonstrated in this work.  It should be cautioned that incremental improvements in prediction accuracy possible through more complex  machine learning strategies should be weighted against the benefits of simply increasing the size of the CFD training data set.  A detailed investigation of crud prediction accuracy vs the training data set size should be conducted in a future study.

Applying the hi2lo technique presented in this work preserves more information about the flow field around spacer grids into the computation of the expected value of crud on each CTF face when compared to subchannel/crud standalone estimates.  The presence of localized hot and cold spots on the rod surface were implicitly accounted for through the reconstruction of the joint density distributions of these fields in each CTF face.  Forgoing the prediction of fine scale spatial details of the surface fields is justified since the crud models employed in this work are one dimensional in nature and do not require intra-CTF face resolved surface fields.

Employment of a Monte Carlo based crud estimation procedure allows for physical intuitions to be built between the sample weights and the physical area represented by a crud sample on the rod surface.  This intuition is especially helpful in the interpretation of the importance weights which result from the application of the importance sampling variance reduction technique.  Additional improvements in sampling efficiency are possible through other variance reduction techniques beyond importance sampling.  To the authors knowledge, this work demonstrates a fist-of-its-kind core-simulator scale Monte Carlo based crud estimation procedure.

One strength of the Monte Carlo based crud procedure is that it is straight forward to propagate hi2lo model uncertainties though time.  Additionally, the Monte Carlo based approach enables extreme value crud event estimation, although, these estimates are expected to carry large uncertainty since extreme upper and lower sample quantiles are plagued by high variance.  Overall this diminishes the applicability of this method to CILC.  

It is not trivial to incorporate feedback between the crud layer and the hi2lo mapping since this would involve making the conditional surface temperature distribution depend on the current crud state.  This crud/thermal hydraulic feedback was presently missed in the current implementation.


\section{Future Work}

A starting point for future hi2lo efforts should be a data scalability study in which the model's predictive performance is characterized as a function of the available training data set size.  Additionally,
the hi2lo model's performance under extrapolation was not investigated in the present work.  The boosted models should not be employed in an extrapolation mode and therefore it is of interest to identify core conditions which would result in evaluating the trained machine learning models outside of the training data envelope at runtime.  It is envisioned that a warning should be raised notifying the user that additional CFD data is required when attempting to evaluate the model outside of thermal-hydraulic zone of applicability.


A complete uncertainty quantification effort should precede efforts to perform a forward uncertainty propagation though the hi2lo model into the crud estimates.  The types of uncertainty should be recognized: Those arising due to the data or measurement based uncertainties and those inherent in the modeling procedures.  Some uncertainties are obvious, such as the uncertainties which arise due to Monte Carlo based integration procedures, however other model uncertainties can be much more difficult to identify.  The CFD data itself presents one source presently unquantified uncertainty.  There exists uncertainty in the conditional quantile predictions made by the boosted machine learning model.  Future studies could consider employing Bayesian additive regression trees for estimating this source of model induced uncertainty.  This could be seen as a form of a variance-of-variances estimation procedure since the goal is to compute confidence intervals for each conditional quantile.
    
Once the sources of uncertainty in the training data and models are accounted for, they must be propagated from the machine learning model predictions through the distribution reconstruction procedures and finally into the crud estimates.  For a credible CILC risk assessment, estimates for the maximum expect crud thickness in addition to the uncertainty in this value are required. Development of this type of uncertainty quantification is required for adoption of this methodology in a production setting.

Future feature engineering efforts should be directed at improving predictive performance under a wide variety of pin orientations and local core conditions.  It is possible to use geometric features such as rod position inside a bundle and bundle position inside the core in addition to the presently considered local thermal hydraulic conditions.  Careful selection of additional of predictive features would improve the ability of the hi2lo model to generalize to previously unseen thermal hydraulic core conditions or unique pin configurations.
	
% Use parametric marginal models in place of the predicted piecewise conditional quantile function.  This would include identifying some class of distribution which accurately models the temperature and TKE variation on a CTF patch.
	
Since application of the present hi2lo model results in an estimate for the crud density distribution on each CTF face it is natural to extend the model towards quantifying CILC risk.  The first objective of a future CILC risk assessment study requires one to derive a CILC risk metric from the hi2lo crud result.  Such a CILC cladding failure probability model could be described by: $\mathcal P_f \propto Pr(C_t(x) > C_t^*)$, where $C_t^*$ is some critical crud thickness and $\mathcal P_f$ is a cladding failure probability.  This would be difficult to quantify with CTF/MAMBA alone and requires either a hi2lo approach or detailed investigation of at-risk pins with high fidelity CFD computations.  A significant challenge is computing an estimate for $Var(\mathcal P_f) = \E[(\mathcal P_f - \E(\mathcal P_f))^2]$, or similarly, the variance in the expected amount of crud over a given threshold.  This quantity is necessary to conduct credible CILC risk assessment.
    
    
% \begin{itemize}
    %\item Explore generating a heat transfer coefficient (HTC) map rather than predicting the temperature residual distribution.  This is avenue for future work is motivated by energy balance consistency concerns.  Under the current strategy the CTF energy balance is disobeyed because the mean surface temperatures are shifted to the CFD value in cases where the $CFD-CTF$ bias is nonzero.  In this scinario the Hi2lo model acts as a replacement for the Dittus-Boelter empirical heat transfer coefficient relation in CTF.
    
    %\item Investigate using a spectral decomposition of the temperature and turbulent kinetic energy on each patch.  The machine learning problem becomes one of predicting the spectra in each CTF face as a function of local core conditions.  One could take advantage of the periodicity in the temperature and TKE surface fields as one traverses the rod in a spiral pattern.  These temperature and TKE spectra can be regarded as an isomorphism of the  joint distribution of temperature and TKE on the rod surface.
	
	% \item Investigate including additional fudge factors in MAMBA crud kinetics to correct crud growth rates near spacer grids.  Tune fudge factors such that MAMBA + CTF to achieve the same integrated crud results as MAMBA ``fine'' + CFD for all entire relevant TH envelope.  This type of problem can be attacked using a standard Bayesian calibration technique.   This strategy would introduce introduce.  If the MAMBA CRUD kinetics change, the tuned fudge factors would need to be regenerated.
	
% \end{itemize}
