\section{Background}

A fundamental difference between the CFD and CTF computations is the average size of the mesh cells.  In the azimuthal coordinate, CTF decomposes a single rod surface into four patches.  An example top down view of typical CFD and CTF meshes for a single pin are given in figure \ref{fig:cfd_ctf_mesh}.  Though both codes employ a finite volume spatial discretization CFD can resolve the flow at much smaller length scales.  Additionally, each code employs a different set of closure models to the underlaying set of coupled energy, mass, and momentum balances.  In practice these differences can lead to large discrepancies in boiling, turbulent mixing, and rod surface temperature predictions between the two codes.

\begin{figure}[!htbp]
\centering
\includegraphics[width=10cm]{images/cfd_ctf_mesh.png}
\caption{Top-down view of typical CFD \& CTF meshes for a single pin \cite{salko12}.}
\label{fig:cfd_ctf_mesh}
\end{figure}

Shown in figure \ref{fig:model_overview}, on a given CTF rod surface patch, a single point estimates for the surface temperature, TKE, and heat flux are predicted.  The predicted CTF quantities are an estimate for the average thermal hydraulic conditions over that coarse patch.   Consequentially, CTF CRUD predictions may significantly deviate from reality.  Since CRUD growth is highly sensitive to the presence of subcooled boiling on the rod surface; if CTF predicts a rod surface temperature less than the saturation point very little or no CRUD will form - when in reality, a small portion of that rod surface could exist above the saturation point and thus harbor CRUD.  Small localized mistakes in CRUD predictions compound throughout the core, leading to poor CIPS estimates. 

In the figure $f$ denotes a probability density function whos value can be interpreted as fractional area of the rod surface.  
\begin{figure}[!htbp]
\centering
\includegraphics[width=12cm]{images/model_relations.png}
\caption{On a single coarse CTF patch: Differences in CRUD prediction between CFD and CTF models.}
\label{fig:model_overview}
\end{figure}

\subsection{Hi2Low Approach}

In this approach the the deterministic and random components of the spatial field are modeled separately.  The deterministic component is supplied by the coarse CTF solution.
The availability of the deterministic portion of the fields of interest via CTF is a boon the proposed Hi2Low methodology.  Typically, additional modeling decisions must be made to construct an estimator for the average behavior of the output fields by a predictor derived by moving average or ordinary least squares regression.  The current methodology is absolved from constructing this predictor.

\begin{equation}
    F(\mathbf x, \mathbf \theta) = \underbrace{\mu(\mathbf{x})}_\text{CTF} + \underbrace{\varepsilon({\mathbf x, \mathbf \theta})}_\text{CFD Informed} + b(\mathbf{x})
\end{equation}
$\varepsilon(x, \theta)$ is the random component of the spatially varying field and can be approximated using a CFD informed model.  $\mathbf \theta$ represents free model parameters that must be learned from the available CFD data.
 The locally averaged quantities, $\mu$, are spatially averaged over a CTF patch.
$b$ is bias present in the mean prediction between the CTF and CFD solutions ($\mu_{CTF} - \mu_{CFD}$).

It would appear that an additional regression step is required to build a predictor for the bias, $b(\mathbf{x})$; however, the bias term can be rolled into the residual term, $\varepsilon$, resulting in residual distributions that are not stationary about zero.
Residual distributions of temperature and turbulent kinetic energy on the rod surface are computed by using the CTF solution to de-trend the CFD born surface fields.  As a consequence, upon evaluation, the proposed Hi2Low model drags the mean predicted surface temperature to the CFD value.  This procedure violates the CTF energy balance.  In the future it may be adventageous to target the heat transfer coefficient rather than the raw surface temperature as a predicted Hi2Low model response.

This issue was partially addressed in the work by Salko et. al. by using a normalized heat transfer coefficient map rather than a temperature map; however, since the surface temperature and heat transfer coefficient are non-trivially related in the subcooled boiling regime it is difficult to rescale the HTC field without disregarding the energy balance on the CTF mesh.  Further investigation into the proper treatment of the bias term is required.  It is not possible to respect the energy balance on both the CFD and CTF meshes when constructing a Hi2Low model in the current fashion.  One possible though expensive solution is to tune out the bias between the codes via calibration of closure models.

\subsection{Capturing Dependence: Copula}

Consider the small patch on a rod's surface shown in figure ().  This patch is treated as a black box in which spatial information is not preserved. Drawing surface samples of the FOI yeilds a correlated temperature, turbulent kinetic energy and boundary heat flux joint distribution.  It is likely that a high temperature corresponds to a low turbulent kinetic energy in this patch, for instance.  This dependence structure is clearly seen in figure ().  The dependence behavior is not fully described by a linear relationship; there exists some dispersion in the joint emperical distributions.  Furthermore, the nature of the dispersion itself is not necissarily gaussian. \\

It would appear that by removing the spatial component of the fields in a patch we have introduced additonal complications by building a joint density function who's shape varies from location to location in the core.  Indeed, this joint density function is non-gaussian and generally ill behaved.  However, by Sklar's theorem this joint distribution can be decomposed into a product of a special function called a copula and univariate probability density functions.  Furthermore, the margins can be modeled indipendently from the copula and later recombined to reconstruct, approximately, the original joint density.  By restricting our attention to a special class of copula governed by a single parameter the original problem of predicting $M+2$ dimensional fields is transformed into a problem of predicting multiple, indipendent, $M+1$ dimensional functions.  $M$ is the number of non-spatial exogenous variables. \\

A two dimensional version of Sklar's theorem is shown in equations () through ().
Given a joint CDF: $H$, w/ cumulative margins: $F(x)=P[X < x] = \int_{-\infty}^{x}f(t)dt$
and $F(y)=P[Y < y] = \int_{-\infty}^{y}f(t)dt$
\begin{equation}
H(x,y) = C(F(x), F(y))
\end{equation}
\begin{equation}
c(u, v) = \frac{\partial^2 C(u, v)}{\partial u \partial v};\ u=F(x), v=F(y)
\end{equation}
Key properties of the copula density function are as follows:
\begin{itemize}
\item  Has uniform marginal density distributions \cite{Nelsen2006}.
\item  Defined on the unit square $[0, 1]^2$ (in the bivariate case)
\end{itemize}
Any joint PDF, $h(\cdot)$ can be decomposed as:
\begin{equation}
h(x, y) = c(F(x), F(y)|\theta)f(x|\theta_x)f(y|\theta_y)
\end{equation}
Where $\theta$ and $\{\theta_{x}, \theta_{y}\}$ are free copula and marginal model parameters respectively.

For the case of Archimedean copula, Kendall's tau, $\rho_\tau$ is
related to the copula's shape parameter by [ref]:
\begin{equation}
\rho_\tau = 1 + 4 \int_0^1 \frac{\varphi(\theta_c,t)}{\varphi'(\theta_c, t)}dt
\end{equation}
Where $\varphi(\theta_c, t)$ is the copula's generator function and $\varphi'$ is the first derivative of the generator function with respect to $t$.  An exaustive list of copula generting functions can be found in [ref].
If we restrict ourselves to the class of Archimedean copula we only need $\rho_\tau$ and the copula type, $\Theta_c$ (gumbel, frank, clayton, ect.) to approximately specify the dependence structure between the temperature and turbulent kinetic energy residual distributions on each patch.

\subsection{Modeling the Margins: Gradient Boosting}

A description of the underlying dependence structure is not sufficient to reconstruct the full joint probability density.  An additional model is required to provide the shape of margins conditioned on the location in the reactor and the local thermal hydraulic conditions.  To this end, a gradient boosted quantile regression model is used.

When composing the complete gradient boosted model from a sequence CART trees the ability to compute the relative importance of each explanatory variableis provided at no additional computational cost.  Given an input variable set of length $N$; each time an split is made orthogonal to an input axis, the model gain (measured by eq () in case of regression and equation () in case of classification) is recorded and placed in a vector of length $N$.  After the CART tree reaches it's maximum depth, the split gain vector is summed.  After boosting is complete and all tree have been grown, the gain vectors are multiplied by the tree weight $\gamma_i$ and summed over all trees.
\begin{equation}
W = g_i
\end{equation}

\subsubsection{Quantile Regression}

The univariate distributions of the temperature and turbulent kinetic energy are shown to be non gaussian in shape in figure ().  The asymitries in the distributions must be predicted accurately by the regression model.  A non-parametric model for the margins is built by combining several quantile predictors.  Leveraging quantile regressions to decompose single dimensional distributions is explained in depth by Oaxaca (1973).

Given a cumulative distribution function (CDF) and a random variable $X$:
\begin{equation}
F_X(x) = P(X \leq x)
\end{equation}
The $\tau^{th}$ quantile $Q$ of $X$ is given by:
\begin{equation}
Q_\tau(X) = F_X^{-1}(\tau)
\end{equation}
Where $F_X^{-1}$ is the inverse cumulative distribution.

The quantile loss function is given by equation ().
\begin{equation}
\rho_\tau(x) = x(\tau - \mathbb{I}_{(x < 0)})
\end{equation}
Where $\mathbb{I}$ is the indicator function.

The quantile loss function is substituted for the squared-error loss function in the gradient boosting algorithm to regress on a qunatile of interest rather than on the mean.  The result of multiple quantile predictores is shown in figure ().  It is important to mention that the model gives conditional quantiles and as such the interpretation of the quantile regression surface sensitivities to changes in the explanatory variable must be carefully made.  The prediced quantiles are conditioned on the location in the core $x_i$, in other words the model provides $Q_T(\tau|x_i)$.  When propogating uncertainties in the exogenous variables to the predicted quantiles it is necissary to compute the sensitivity of each quantile, $\tau_j$ at some location $x_i$ to small purtubations in each of the covariates.
\begin{equation}
\beta_{\tau_{ij}} = \frac{\partial Q_T(\tau_j|x_i)}{\partial x_i}
\end{equation}

A one dimensional slice from the multiple regression is provided in figure ().
The resulting quantiles are used to construct a step-wise cumulative distribution, which in turn is used to build a histogram of the predicted quantity of interest. \\

Shown in figure (), in place of the stepwise reprentation, a piecewise cubic hermite interpolating polynomial (PHCIP) can be fit to the discrete quantile estimate to generate a differentiable CDF.
It can be shown that PCHIP interpolation preserves monotinicity of the CDF if the provided quantiles are strictly monotone [].  This condition is enforced in the software, and any violation of the monotone restriction would indicate a software bug in the quantile regression code.
