\section{Background}

A fundamental difference between the CFD and CTF computations is the average size of the mesh cells.  In the azimuthal coordinate, CTF decomposes a single rod surface into four patches.  An example top down view of typical CFD and CTF meshes for a single pin are given in figure \ref{fig:cfd_ctf_mesh}.  Though both codes employ a finite volume spatial discretization CFD can resolve the flow at much smaller length scales.  Additionally, each code employs a different set of closure models to the underlying set of coupled energy, mass, and momentum balances.  In practice these differences can lead to large discrepancies in boiling, turbulent mixing, and rod surface temperature predictions between the two codes.

\begin{figure}[!htbp]
\centering
\includegraphics[width=10cm]{images/cfd_ctf_mesh.png}
\caption{Top-down view of typical CFD \& CTF meshes for a single pin \cite{salko12}.}
\label{fig:cfd_ctf_mesh}
\end{figure}

Shown in figure \ref{fig:model_overview}, on a given CTF rod surface patch, a single point estimates for the surface temperature, TKE, and heat flux are predicted.  The predicted CTF quantities are an estimate for the average thermal hydraulic conditions over that coarse patch.   Consequentially, CTF CRUD predictions may significantly deviate from reality.  Since CRUD growth is highly sensitive to the presence of subcooled boiling on the rod surface; if CTF predicts a rod surface temperature less than the saturation point very little or no CRUD will form - when in reality, a small portion of that rod surface could exist above the saturation point and thus harbor CRUD.  Small localized mistakes in CRUD predictions compound throughout the core, leading to poor CIPS estimates. 

In the figure $f$ denotes a probability density function whos value can be interpreted as fractional area of the rod surface.  
\begin{figure}[!htbp]
\centering
\includegraphics[width=12cm]{images/model_relations.png}
\caption{On a single coarse CTF patch: Differences in CRUD prediction between CFD and CTF models.}
\label{fig:model_overview}
\end{figure}

\subsection{Hi2Low Approach}

In this approach the FOI are split into deterministic and random components where the coarse CTF solution supplies the former and the CFD results provides the latter.
The availability of the deterministic portion of the fields of interest via CTF is a boon the proposed Hi2Low methodology.  In a pure regression setting in which only CFD data is available, an additional modeling step to construct an estimator for the average behavior of the output fields would be necessary.  This predictor would be derived by moving average or ordinary least squares regression.  However, the current methodology is absolved from constructing this predictor.

\begin{equation}
    F(\mathbf z, \mathbf q) = \underbrace{\mu(\mathbf{z})}_\text{CTF} + \underbrace{\varepsilon({\mathbf z, \mathbf \theta(\mathbf q)})}_\text{CFD Informed} + b(\mathbf{z})
\end{equation}
Where $\mathbf z$ represents the spatial coordinates on the rod surface and $\mathbf q$ are a set of auxiliary predictor variables that represent local-averaged core conditions.  In this work $\mathbf q$ is taken to be the surface $T, q''$ and $TKE$ fields supplied by CTF. $\varepsilon(z, \theta(\mathbf q))$ is the random component of the spatially varying field and can be trained using CFD data sets.  $\mathbf \theta(\mathbf q)$ represent copula and marginal model parameters that must be learned from the available CFD data.  These free parameters vary as a function of the auxiliary predictors.
 The quantities, $\mu(z)$, are spatially averaged over a CTF patch.
$b(z)$ is bias present in the mean predictions between the CTF and CFD solutions ($b(z) = \mu_{CTF}(z) - \mu_{CFD}(z)$).

It would appear that an additional regression step is required to build a predictor for $b(\mathbf{z})$; however, the bias term can be rolled into the residual term, $\varepsilon$, resulting in residual distributions that are not stationary about zero.
Residual distributions of temperature and turbulent kinetic energy on the rod surface are computed by using the CTF solution to de-trend the CFD born surface fields.  As a consequence, upon evaluation, the proposed Hi2Low model drags the mean predicted surface temperature to the CFD value.  Unfortunately, this procedure violates the CTF energy balance.  In the future it may be advantageous to target the heat transfer coefficient rather than the raw surface temperature as a predicted Hi2Low model response.

This issue was partially addressed in the work by Salko et. al. by using a normalized heat transfer coefficient map rather than a temperature map; however, since the surface temperature and heat transfer coefficient are non-trivially related in the subcooled boiling regime it is difficult to justify rescaling the HTC field without disregarding the energy balance satisfied on the CFD mesh.  Further investigation into the proper treatment of the bias term is required.  It is not possible to respect the energy balance simultaneously on the CFD and CTF meshes when constructing a Hi2Low model in the current fashion.  One possible though expensive solution is to tune out the bias between the codes via calibration of closure models.

The remainder of the section describes the models used to build the CFD informed ($\varepsilon$) term.

\subsection{Capturing Dependence: Copula}

Consider the small patch on a rod's surface shown in figure \ref{fig:ctf_patch_dist}.  This patch is treated as a black box in which spatial information is not preserved. Drawing surface samples from a CFD simulation of the FOI yields a correlated temperature, turbulent kinetic energy and boundary heat flux joint distribution.  It is likely that a high temperature corresponds to a low turbulent kinetic energy in this patch, for instance.  This dependence structure is clearly seen in figure \ref{fig:ctf_patch_dist}.  The dependence behavior is not fully described by a linear relationship; there exists some dispersion in the joint empirical distributions.  Furthermore, the nature of the dispersion is non-Gaussian.  

The proposed method revolves around tracking the the joint $f(T, TKE, q'')$ distribution given local TH core conditions, $\mathbf q$, provided by CTF. \\

\begin{figure}[!htbp]
\centering
\includegraphics[width=18cm]{images/ctf_patch_ex3.png}
\caption{Relationships between surface temperature [K], TKE [$J/kg$], boron mass density $g/cm^2$, and CRUD thickness [microns]. Generated from CFD simulation tallied over (259.7, 267.0)[cm]. Results were generated at 160 percent nominal power in a single pin configuration \cite{slattery16}.}
\label{fig:ctf_patch_dist}
\end{figure}

It would appear that by removing the spatial component of the fields in a patch we have introduced additional complications by building a joint density function who's shape varies from location to location in the core.  Indeed, this joint density function is non-Gaussian and generally ill behaved.  However, by Sklar's theorem this joint distribution can be decomposed into a product of a special function called a copula and uni variate probability density functions.  Furthermore, the margins can be modeled independently from the copula and later recombined to reconstruct, approximately, the original joint density.  By restricting our attention to a special class of copula governed by a single parameter the original problem of predicting $M+2$ dimensional fields is transformed into a problem of predicting multiple, independent, $M+1$ dimensional functions.  $M$ is the number of non-spatial exogenous variables. \\

A bivariate copula CDF is given by equation \ref{eq:2d_sklar}.  The multi-dimensional case and additional details are provided in Appendix A.  Let $x$ represent temperature and $y$ be the turbulent kinetic energy.
Given a joint CDF of these two quantities, $H$, with cumulative temperature and TKE margins: $F(x)=P[X < x] = \int_{-\infty}^{x}f(t)dt$
and $F(y)=P[Y < y] = \int_{-\infty}^{y}f(t)dt$. Sklar's theorem states \cite{Nelsen2006}:
\begin{equation}
H(x,y) = C(F(x), F(y))
\label{eq:2d_sklar}
\end{equation}
Where $C$ is the cumulative distribution of a copula.  The copula density can be computed from equation \ref{eq:cop_density}.
\begin{equation}
c(u, v) = \frac{\partial^2 C(u, v)}{\partial u \partial v};\ u=F(x), v=F(y)
\label{eq:cop_density}
\end{equation}

It can be shown by that any joint PDF, $f$, can be decomposed as:
\begin{equation}
f(x, y) = c(F(x), F(y)|\theta_c)f(x|\theta_x)f(y|\theta_y)
\end{equation}
Where $\theta_c$ and $\{\theta_{x}, \theta_{y}\}$ are free copula and marginal model parameters respectively.
These parameters govern the shape of the joint distribution.  In a fully parametric approach, these shape parameters would be specified as a function of local core conditions; however, since CFD data gives rise to complicated distribution shapes, a semi-parametric approach is proposed.  Instead of assuming some distributions - beta, Gaussian, ect. - the proposed strategy is to reconstruct the margins via quantile regression.  In the current work the quantile regressions are built from gradient boosted regression trees (GBRT). 

In addition, a regression model for the copula shape parameter $\theta_c$ is needed, though we will show this can be substituted for a regression on Kendall's tau.  Similar to the quantile regression strategy, a gradient boosted regression tree model is proposed for this purpose.  Finally, the copula family must also be predicted.  This gives rise to a classification problem in which the copula type that best fits the temperature, TKE dependence structure is predicted based on local core conditions.  Thus the model isn't fully non-parametric.

If the quantiles of the margins and the properties of the copula can be specified as a function of local core conditions then the full joint distribution can be recovered on any CTF patch.

In this work, a particular property of a a certain class of copula functions is utilized.
For the case of Archimedean copula, Kendall's tau, $\rho_\tau$ is
related to the copula's shape parameter by equation \ref{eq:kt} \cite{Nelsen2006}:
\begin{equation}
\rho_\tau = 1 + 4 \int_0^1 \frac{\varphi(\theta_c,t)}{\varphi'(\theta_c, t)}dt
\label{eq:kt}
\end{equation}
Where $\varphi(\theta_c, t)$ is the copula's generator function and $\varphi'$ is the first derivative of the generator function with respect to $t$.  An exhaustive list of copula generating functions can be found in \cite{Nelsen2006}.
If we restrict ourselves to the class of Archimedean copula we only need $\rho_\tau$ and the copula type, $\Theta_c$ (gumbel, frank, clayton, ect.) to approximately specify the dependence structure between the temperature and turbulent kinetic energy residual distributions on each patch.  Therefore, the gradient boosted copula model predicts $\rho_\tau$ as a function of local core conditions rather than $\theta_c$.

\subsection{Modeling the Margins: Gradient Boosting}

A description of the underlying dependence structure is not sufficient to reconstruct the full joint probability density.  An additional model is required to provide the shape of margins conditioned on the location in the reactor and the local thermal hydraulic conditions.  A gradient boosted quantile regression model fills this role.

Gradient boosting is an ensemble technique in which a sequence of weak learners are fit to the training data in a stagewise fashion.  Each successive weak learner is specialized to correct the errors made by the previous learners in the ensemble.  A weighted sum over all the weak learner predictions results in the final ensemble prediction.  Making a predictions from an ensemble of weak learners is analogous to taking a weighted vote from a crowd with members each producing marginally better than random results - similar a certain game show's ``ask the audience lifeline''.  

As in the application of any machine learning technique, care must be taken to minimize over fitting the data.  If unmitigated, outliers can have a massive impact on the predictions make by the trained model.  Regularization techniques developed by Breiman and Friedman known as shrinkage and bagging reduce the importance of outliers on the boosted model \cite{breiman1996}.  In the machine learning context, regularization refers to any technique which adds additional penalizing terms to the model in order to prevent over fitting.  The general idea is related to the use of Lagrange multipliers in solving constrained optimization problems.  An introduction to the gradient boosting algorithm is provided in Appendix C.

In this work, gradient boosted decision trees are used for the regression of surface TKE and temperature quantiles and the classification of copula based on local core conditions.   Each decision tree plays the role of a weak learner.  A decision tree take the form of a binary tree.  Each node in the tree represents a partition in the input space that is orthogonal to an explanatory variable axis.  In each split region, a constant predictive value is assigned; typically the mean value is used in a regression setting or the most likely class label in a classification setting.  The decision tree's used in this work are formally known as classification and regression (CART) trees.  A detailed description of CART trees and their application to ecological classification problems is provided by G. Death and K. Fabricious (2000) \cite{death2000}. 

% \subsubsection{Variable Importance}
%When composing the gradient boosted model from a sequence CART trees it is possible to estimate the relative importance of each explanatory variable at no additional computational cost.  Given an input variable set of length $N$; each time an split is made orthogonal to an input axis, the model gain (measured by eq () in case of regression and equation () in case of classification) is recorded and placed in a vector of length $N$.  After the CART tree reaches it's maximum user specified depth, the nodes' split gain vectors are summed.  After boosting is complete and all tree have been grown, the tree gain vectors are multiplied by their respective weights, $\gamma_i$, and summed over all trees.
%\begin{equation}
%W = g_i
%\end{equation}
%The end result measures of how often each explanatory axis was chosen for splitting weighted by the gain of the splits.

\subsubsection{Quantile Regression}

The uni-variate distributions of the temperature and turbulent kinetic energy are shown to be non Gaussian in figure \ref{fig:ctf_patch_dist}.  The asymmetries in these distributions must be accurately preserved by the regression model.  To accomplish this, a non-parametric model for the margins is built from predicted conditional quantiles.  Leveraging quantile regressions to reconstruct conditional distributions has been described previously by Oaxaca (1973) and Koenker (2005) \cite{koenker05}.

Given a cumulative distribution function (CDF) and a random variable $X$:
\begin{equation}
F_X(x) = P(X \leq x)
\end{equation}
The $\tau^{th}$ quantile $Q_\tau$ of $X$ is given by equation \ref{eq:th_quantile}.  The most well known quantile is the median.
\begin{equation}
Q_\tau(X) = F_X^{-1}(\tau)
\label{eq:th_quantile}
\end{equation}

The quantile loss function is given by equation \ref{eq:qt_loss_a} \cite{koenker05}.
\begin{equation}
l_\tau(u) = u \cdot (\tau - \mathbb{I}_{(u < 0)})
\label{eq:qt_loss_a}
\end{equation}
Where $\mathbb{I}$ is the indicator function and $u=X - \epsilon$.
Given sample data $\{x_0, ... x_n\}$ (distributed according to $F_X$), the $\tau^{th}$ sample quantile can be computed by minimizing equation \ref{eq:qt_loss_a} wrt. $\epsilon$:

\begin{equation}
\hat Q_\tau = \mathrm{min}_\epsilon \sum_{i=1}^n l_\tau (x_i - \epsilon)
\end{equation}

\begin{figure}[!htbp]
\centering
\includegraphics[width=6cm]{images/q_loss.png}
\caption{Quantile loss function.  Reproduced from \cite{koenker05}.}
\label{fig:ctf_patch_dist}
\end{figure}

The quantile loss function is substituted for the squared-error loss function in the gradient boosting algorithm to regress on a qunatile of interest rather than on the mean.  

% A one dimensional slice from the quantile regression is provided in figure ().
The resulting quantiles are used to construct a step-wise cumulative distribution, which in turn is used to build a histogram. 
In place of the stepwise representation, a piecewise cubic hermite interpolating polynomial (PHCIP) can be fit to the stepwise conditional quantile distribution to generate a differentiable CDF.
The PCHIP interpolation preserves monotonicity of the CDF if the provided quantiles are strictly monotone.  This condition is enforced in the software; any violation of the monotone restriction would indicate a software bug in the quantile regression code.
