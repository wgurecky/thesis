%-----------------------------------------------------------------------------%
\section{Copula}

In this section, Sklar's theorem is provided along with examples of copula functions
and techniques to draw samples from them.  Several definitions are presented
to define all the requisite notation.

The product rule of probability is shown in equation \ref{eq:prod_prob}.  The comma denotes the conjunction
``and'' and the bar, $|$, is read ``given''.

\begin{equation}
P(x, y) = P(x) \cdot P(y | x)
\label{eq:prod_prob}
\end{equation}

The definition of a marginal distribution is given in equation \ref{eq:marg}.  The marginalization process is analagous to projecting the entire joint density onto a single axis. 

\begin{align*}
P(x) &= \int P(y) P(x|y) dy \\ P(y) &= \int P(x) P(y|x) dx
\label{eq:marg}
\end{align*}

The cumulative density function, $F$ is defined as:

\begin{align*} 
F &= P[X < x] = \int_{-\infty}^x P(x)dx \\
\end{align*}

A joint $d$ dimensional cumulative distribution is given by equation \ref{eq:joint_cdf}.

\begin{equation}
H(x_1, ... x_d) = \mathbf P[X_1 \leq x_1, ... X_d \leq x_d]
\label{eq:joint_cdf}
\end{equation}
Where $X_1, ... X_d$ are random variables.

The process of decomposing a multivariate distribution into univariate marginal
distributions and an object which describes their conditional dependence was
formalized by Sklar (1959) \cite{Sklar1959}.  Shown in equation \ref{sklar1},  Sklar's Theorem
defines a \emph{copula} cummulative density function, $C$.

\begin{equation}
C(F_1(x_1), ... F_d(x_d)) = H(x_1, ... x_d)
\label{sklar1}
\end{equation}
If $F_1, .. F_d$ are continuuous, then $C$ is unique.  Conversely, if $C$ is a copula and $F_1, .. F_d$ are smooth cummulative desntiy functions then the function $H$ is a joint cumulative distribution with margins $F_1, ... F_d$.

Sklar also showed that the joint probability distribution, $f(x_1, ... x_d)$, can be computed from
constituent marginalized univariate distributions and the \emph{copula} density, $c$.

\begin{equation}
f(x_1,\dots x_d)= c(F_1(x_1),\dots F_d(x_d))\cdot f_1(x_1)\cdot\dots\cdot f_d(x_d)
\label{sklar2}
\end{equation}

For brevity, let $u_1, .. u_d$ represent samples from their CDFs as follows:
\begin{align*} u_1 &= F_1(x_1) \\ u_d &= F_d(x_d) \\ u &\in
[0, 1]
\end{align*}

Where the joint density of the copula, $c$, is given by equation \ref{eq:cop_pdf}:
\begin{equation}
c(u_1, ... u_d) = \frac{\partial C(u_1, ... u_d)}{\partial u_1 ... \partial u_d}
\label{eq:cop_pdf}
\end{equation}

The power of Sklar's theorem resides in the ability to construct
models for the margins separately from a model of the dependence structure.
When combined, the margins and the copula completely
specify any multivariate probability density function.
Compared to rudimentary approaches based on an assumed linear dependence structure,
a copula based approach can treat skewed dependence structures in which the
strength of dependence (as measured via a local correlation coefficient) is allowed
to vary depending on location in the parameter space.

\section*{Sampling Copula}

For simplicity, this section demonstrates how to draw correlated samples from bivariate copula.
Sampling from a bivariate copula is achived by first defining a conditional distirbution fuction and then applying the inverse probability transform.
Let $h$ represent the conditional distribution of $u_1$ given $\{u_2, ... u_d\}$.  In the two dimensional case $h$ is given by equation \ref{eq:cop_h}:

\begin{equation}
h(u_1 | u_2) = \frac{\partial C(u_1, u_2)}{\partial u_2}
\label{eq:cop_h}
\end{equation}

If the univariate distribution $h$ is smooth and monatonic the inverse is unique.
To sample from a bivariate copula, first a random vector of length $N$ is drawn from the uniform distribution $\in [0, 1]$:  $\{\mathbf U_2\}$.  For each sample, $u_{2_i}$ in $\{\mathbf U_2\}$ the 1D line search problem given in equation \ref{eq:h_inv_sample} is solved.  This produces a sample vector of length $N$: $\{\mathbf U_1\}$.

\begin{equation}
u_{1_i} = \mathrm{argmin}_{x} \left[ h(x|u_{2_i}) - u_{2_i} \right],\ \mathrm{with}\ 0 < x < 1
\label{eq:h_inv_sample}
\end{equation}

The resulting corrolated sample vectors $\{\mathbf U_1, \mathbf U_2\} \in [0,1]^2$ are distributed according to the copula, $c$, and have uniform margins.  An example of random samples drawn from a Gaussian copula are shown in figure ().

To apply arbitrary margins we employ, again, the inverse probability transform.  As before, the cummulative marginal densities are given to be $F_1$ and $F_2$.
Correlated samples are then drawn according to:
\begin{eqnarray}
X = & F_1^{-1}(\mathbf U_1) \\
Y = & F_2^{-1}(\mathbf U_2) \\
\end{eqnarray}
The samples vectors $\{X, Y\}$ are distributed according to the joint density, $f$.  An example bivariate sample set with exponentially distributed margins and a gaussian copula is shown in figure ().

\section*{Copula Families}

\subsection*{Elliptic}

Provided an Elliptic copula lines of constant probability density form ellipses.

This family of copula are unsuitable for capturing skewed dependence structures.

\subsection*{Archimedian}

Archimedian copula are defined by the relationship.

Where $\psi$ is a monotonically decreasing and convex function.

Archimedian copula are not necessarily symmetric, and therefore allow one to capture
skewed dependence structures.

\section*{Fitting Copula}