%-----------------------------------------------------------------------------%
\section{Copula}

In this section, Sklar's theorem is provided along with examples of copula functions
and techniques to draw samples from them. 

The product rule of probability is shown in equation \ref{eq:prod_prob}.  To clarity notation used in this section : The comma denotes the conjunction ``and'' and the bar, $|$, is read ``given''.
\begin{equation}
P(x, y) = P(x) P(y | x)
\label{eq:prod_prob}
\end{equation}

The marginal distribution of a bivariate joint distribution $f(x, y)$ is given by equation \ref{eq:marg}.  The marginalization process is analogous to projecting the entire joint density onto a single axis.
\begin{equation}
f(x) = \int f(y) f(x|y) dy
\label{eq:marg}
\end{equation}

The cumulative density function, $F$ is defined as:
\begin{align*} 
F &= \mathbf{P}[X < x] = \int_{-\infty}^x f(x)dx \\
\end{align*}

A joint $d$ dimensional cumulative distribution is given by equation \ref{eq:joint_cdf}.
\begin{equation}
H(x_1, ... x_d) = \mathbf P[X_1 \leq x_1, ... X_d \leq x_d]
\label{eq:joint_cdf}
\end{equation}
Where $X_1, ... X_d$ are random variables.

The process of decomposing a multivariate distribution into uni-variate marginal
distributions and an object which describes their conditional dependence was
formalized by Sklar (1959) \cite{Sklar1959}.  Shown in equation \ref{eq:sklar1},  Sklar's Theorem
defines a \emph{copula} cumulative density function, $C$.

\begin{equation}
C(F_1(x_1), ... F_d(x_d)) = H(x_1, ... x_d)
\label{eq:sklar1}
\end{equation}
If $F_1, .. F_d$ are continuous, then $C$ is unique.  Conversely, if $C$ is a copula and $F_1, .. F_d$ are smooth cumulative destiny functions then the function $H$ is a joint cumulative distribution with margins $F_1, ... F_d$.  A proof is provided in Nelsen's introductory copula text \cite{Nelsen2006}.

Sklar also showed that the joint probability distribution, $f(x_1, ... x_d)$, can be computed from
constituent marginalized univariate distributions and the copula density, $c$.
\begin{equation}
f(x_1,\dots x_d)= c(F_1(x_1),\dots F_d(x_d))\cdot f_1(x_1)\cdot\dots\cdot f_d(x_d)
\label{eq:sklar2}
\end{equation}

For brevity, let $u_1, .. u_d$ represent samples from their CDFs as follows:
\begin{align*} u_1 &= F_1(x_1) \\ u_d &= F_d(x_d) \\ u &\in
[0, 1]
\end{align*}

Where the joint density of the copula, $c$, is given by equation \ref{eq:cop_pdf}:
\begin{equation}
c(u_1, ... u_d) = \frac{\partial C(u_1, ... u_d)}{\partial u_1 ... \partial u_d}
\label{eq:cop_pdf}
\end{equation}

The power of Sklar's theorem resides in the ability to construct
models for the margins separately from a model of the dependence structure.
When combined, the margins and the copula completely
specify any multivariate probability density function.
Compared to rudimentary approaches based on covariance matrix dependence model,
a copula based approach can treat skewed dependence structures in which the
strength of dependence is allowed to vary depending on location in the parameter space.

\section*{Sampling Copula}

For simplicity, this section demonstrates how to draw correlated samples from bivariate copula.
Sampling from a bivariate copula is achieved by first defining a conditional distribution function and then applying the inverse probability integral transform.
Let $h$ represent the conditional distribution of $u_1$ given $\{u_2, ... u_d\}$.  In the two dimensional case $h$ is given by equation \ref{eq:cop_h} \cite{Nelsen2006}:

\begin{equation}
h(u_1 | u_2) = \frac{\partial C(u_1, u_2)}{\partial u_2}
\label{eq:cop_h}
\end{equation}

If the distribution $h$ is smooth and monotonic, the inverse $h^{-1}$ exists.  These functions are shown in figures \ref{fig:gauss_h} and \ref{fig:gauss_hinv} for several values of the conditioning variable ($u_2$) and a gaussian copula with a shape parameter $\theta=0.7$.  

\begin{figure}[!htbp]
\centering
\begin{minipage}{.45\textwidth}
  %
  \includegraphics[width=7cm]{images/t_h_dist.png}
\caption{The conditional $h$ \\ function vs. value of the \\ conditioning variable $u_2$ \\ for a gaussian copula with $\theta=0.7$.}
\label{fig:gauss_h}
\end{minipage}%
\begin{minipage}{.45\textwidth}
  %
  \includegraphics[width=7cm]{images/t_hinv_dist.png}
\caption{$h^{-1}$ vs. value of the conditioning variable $u_2$ for a Gaussian copula with $\theta=0.7$.\\}
\label{fig:gauss_hinv}
\end{minipage}
\end{figure}

Computing the inverse analytically is oftentimes not possible for some classes of copula and therefore, the more general method shown in equation \ref{eq:h_inv_sample} is used.
A random vector of length $N$ is drawn from the uniform distribution $\in [0, 1]$:  $\{\mathbf U_2\}$.  For each sample, $u_{2_i}$ in $\{\mathbf U_2\}$ the 1D line search problem given in equation \ref{eq:h_inv_sample} is solved.  This produces a sample vector of length $N$: $\{\mathbf U_1\}$.

\begin{equation}
u_{1_i} = \mathrm{argmin}_{x} \left[ h(x|u_{2_i}) - u_{2_i} \right],\ \mathrm{with}\ 0 < x < 1
\label{eq:h_inv_sample}
\end{equation}

The resulting correlated sample vectors $\{\mathbf U_1, \mathbf U_2\} \in [0,1]^2$ are distributed according to the copula, $c$, and have uniform margins.  An example of random samples drawn from a Gaussian copula are shown in figure \ref{fig:gauss_samples}.  The smooth Gaussian copula PDF is provided in figure \ref{fig:gauss_pdf}.

\begin{figure}[!htbp]
\centering
\begin{minipage}{.45\textwidth}
  %
  \includegraphics[width=9.2cm]{images/gauss_copula_pdf.png}
\caption{Gaussian copula density\\ with $\theta=0.7$.}
\label{fig:gauss_pdf}
\end{minipage}%
\begin{minipage}{.45\textwidth}
  %
  \includegraphics[width=7cm]{images/gauss_samples.png}
\caption{Samples drawn from gaussian copula\\ with $\theta=0.7$.}
\label{fig:gauss_samples}
\end{minipage}
\end{figure}

To apply arbitrary margins we employ, again, the inverse probability transform.  As before, the cumulative marginal densities are given to be $F_1$ and $F_2$.
Correlated samples are then drawn according to:
\begin{eqnarray}
\mathbf X = & F_1^{-1}(\mathbf U_1) \\
\mathbf Y = & F_2^{-1}(\mathbf U_2)
\end{eqnarray}
The samples vectors $\{\mathbf X, \mathbf Y\}$ are distributed according to the joint density, $f(x,y)$.  An example bivariate sample set with exponentially distributed margins and a Gaussian copula is shown in figure \ref{fig:gauss_samples_scaled}.  In the example figure the exponential marginal distributions are given by $f(x)=\lambda e^{-\lambda x}$ with $\lambda=2\mathrm{E-}3$.

\begin{figure}[!htbp]
\centering
\includegraphics[width=9cm]{images/gauss_samples_scaled.png}
\caption{Samples drawn from gaussian copula with exponential margins.}
\label{fig:gauss_samples_scaled}
\end{figure}


\section*{Copula Families}

A wide range of copula functions are available in the literature.  In order to satisfy the definition of a copula several criterion must be met:
\begin{enumerate}
\item Must integrate to one on $[0, 1]^n$
\item Must have uniform marginal distributions (as shown in figure \ref{fig:gauss_samples}).
\item When one argument to the joint copula CDF is zero, the CFD is zero:
\begin{equation}
C(u_1, u_2, ... 0, ... u_d) = 0
\end{equation}
\item When one argument to the joint copula CDF is $u\in[0,1]$ and all other arguments are one, the CFD is takes a value equal to $u$:
\begin{equation}
C(1, 1, ... u, ... 1) = u
\end{equation}
\end{enumerate}

Examples of valid copula are given in figure \ref{fig:montage_cop}.  A wide range of skewed dependence structures can be represented by considering only a few copula families.  Each copula can be rotated to accommodate both positive or negative dependence.

\begin{figure}[!htbp]
\centering
\includegraphics[width=18cm]{images/montage_copula_pdf.png}
\caption{Examples of bivariate copula PDFs.}
\label{fig:montage_cop}
\end{figure}

\subsection*{Fitting Copula}

Fitting copula to empirical data can be carried out by the method of maximum likelihood (ML).  Consider the bivariate case where there are $N$ pairs of empirical ranked data samples $\{w_i, v_i\}$ $\in [0,1]^2$ are known. The likelihood function for a copula is given by equation \ref{eq:lik}.  For any given sample pair, each term the likelihood function describes how probable it was for that sample originated from the underlying copula distribution with parameter $\theta$.
\begin{equation}
\mathcal{L}= \prod_{i=0}^N c(w_i, v_i|\theta)
\label{eq:lik}
\end{equation}

Where $\theta$ is the free copula shape parameter.
Typically the negative log-likelihood given by equation \ref{eq:log_lik} is used in place of the raw likelihood.  
\begin{equation}
-\mathrm{ln}\mathcal{L}= -\sum_{i=0}^N c(w_i, v_i|\theta)
\label{eq:log_lik}
\end{equation}
To minimize the negative log likelihood one simply computes the partial derivative with respect to $\theta$ and finds the value $\hat \theta_{ML}$ for which this expression reaches zero.  This can carried out by newtons method.  If the partial derivatives of the copula's negative log likelihood are difficult to compute one can estimate them by finite difference. 

However one important question to ask is: Which copula family best represents the data?  To answer this question, an arsenal of statistical tests can be applied to select the copula which best fits the data.  Here, we consider two methods:  (1) Comparing Akaike information criterion (AIC) and (2) graphically comparing each fitted copula.  

The AIC is computed by equation ():
\begin{equation}
\mathrm{AIC} = 2k - 2\mathrm{ln}(\mathcal{L})
\end{equation}
Where $k$ is the number of free parameters in the model.  The AIC penalizes models with larger numbers of parameters. 
Automated copula selection is achieved by selecting the copula that obtains the lowest AIC score.

A graphical method of copula selection was proposed by [ref].  In this method, the 
\begin{equation}
K_c = 
\end{equation}

\begin{figure}[!htbp]
\centering
\begin{minipage}{.45\textwidth}
  %
  \includegraphics[width=6.5cm]{images/original_stocks.png}
\caption{Ficticious bivariate \\ data set.}
\label{fig:biv_data_ex}
\end{minipage}%
\begin{minipage}{.45\textwidth}
  %
  \includegraphics[width=9cm]{images/ktau_function_plot1.png}
\caption{Graphical comparison of  \\ kendall's distribution for \\ several fitted copula.}
\label{fig:kc_fn_compare}
\end{minipage}
\end{figure}
By graphical inspection, the Gumbel copula is the best fit to the original data set.

\subsection*{Kendall's Tau}

Kendall's tau is a measure of concordance.  Consider two correlated and ranked random variables, $X, Y$.
If a rank $x_i$ is drawn for the first variable, the likelihood that you will also have drawn the rank $y_i$ of the second to be at least as great as the first is proportional to Kendall's tau.  More precisely, let $(X_i, Y_i)$ and $(X_j, Y_j)$ be identically distributed sample pairs from some joint cumulative distribution so that $X\ and\ Y$ will take on values in $[0,1]$, then Kendall's tau is given by equation \ref{eq:ktau} \cite{Nelsen2006}.  

\begin{equation}
\rho_\tau = P[(X_i - X_j)(Y_i-Y_j)>0] - P[(X_i - X_j)(Y_i - Y_j)<0]
\label{eq:ktau}
\end{equation}

In the case of Archimedean copula, $\rho_\tau$ is directly related to the copula's parameter, $\theta$.
Equation \ref{eq:tauar} relates an Archimedean copula's parameter to $\rho_\tau$.  This is useful since if one can estimate $\rho_\tau$ from the empirical data and the copula type is known, one can quickly compute the copula's shape parameter without resorting to the method of ML.

\begin{equation}
\rho_\tau = 1 + 4 \int_0^1 \frac{\varphi(\theta,t)}{\varphi'(\theta, t)}dt
\label{eq:tauar}
\end{equation}
Where $\varphi(t)$ is the copula's generator function and $\varphi'(t)$ is the first derivative of the generator function with respect to $t$. A list of copula generator functions can be found in \cite{Nelsen2006}. 

%\subsection*{Archimedean Copula}

%Archimedean copula are defined by the relationship.

%Where $\psi$ is a monotonically decreasing and convex function.

%A simplification can be made when fitting Archimedean copula since there is a one-to-one relationshipt between $\rho_\tau$ and the copula's shape parameter, $\theta$.
