\section{Hi2Low Literature Overview}

The practice of utilizing CFD and experimental data to improve subchannel flow models is not a new practice.  Though the current Hi2Low strategy is targeted specifically to capture spacer grid effects on CRUD growth, it is in a similar vein to these past CFD informed subchannel efforts.  The following examples of previously conducted Hi2Low work focused on accounting for spacer grid effects on the heat transfer coefficient and on inter channel mixing. 

Over time the CTF code has accumulated many incremental improvements.  A recent improvement targeted the treatment spacer grid induced cross-flow was devised by M. Avramova (2007) \cite{avramova2007}. 
The approach put forth in this work involved tuning coefficients of auxiliary terms included in the momentum and energy balances implemented in CTF.  These additional terms approximately capture fine scale turbulent mixing effects introduced by the presence of the spacer grid arrays on the bulk flow.  Semi-mechanistic models for the (mixing) coefficients were fit to data supplied by CFD computations.  Substantial improvment in bundle outlet temperature distributions from the base model was demonstrated with no additional computational overhead.  The nature of this work required large changes to the CTF source code as the momentum and energy balance equations were modified to accomidate the new spacer grid effects.   This work is representative of the state-of-the-art in utilizing CFD predictions to improve subchannel models, however the methods employed here were not focused on retaining fine scale detail from the CFD predictions as is required in the current work.

A similar strategy has been employed to capture grid-enchanced heat transfer in subchannel codes.   A function parameterized by vane angle is used as a multiplier map to adjust the heat transfer coefficent to reflect the presence of the spacer grids.  This YHL model is used in CTF to improve departure from nucleate boiling (DNB) predictions.  Due to CTF's coarse spatial discritization, the code cannot explicitly resolve the small scale voticies responsible for the increased heat transfer on the rod surface without these empirical enchancement functions.   

A Hi2Low stratagy proposed by Salko et. al (2017) specifically targets capturing spacer grid effects on CRUD growth \cite{salko17}.  This work is of particular importance because it represents a direct alternative to the proposed statistically based Hi2Low model for the purposes of enhancing the accuracy of CRUD predictions at the core-wide scale.  The model relies on mapping CFD born spatial distributions of the heat transfer coefficent (HTC) and surface TKE onto an intermediate grid upon which CRUD is grown.  The intermediate grid sits in between the CFD and CTF meshes and it's resolution is tunable.  An illustration of the intermideate grid setup is given in figure \ref{fig:cfd2ctf_map}.  In the figure, the tubulent kinetic energy is mapped on to the intermediate grid and is normalized by the CTF TKE prediction before passing the reconstructed TKE distribution to the CRUD simulator as a boundary condition. 

\begin{figure}[!htbp]
\centering
\includegraphics[width=8cm]{images/cfd_ctf_multi_grid.png}
\caption{Mapping from the CFD grid to the reconstruction (CRUD) grid.  Reproduced from \cite{salko17}.}
\label{fig:cfd2ctf_map}
\end{figure}

In order to obey energy conservation on the target grid,  an iterative method was used to converge the temperature field on target CRUD grid since the boundary heat flux is non-trivially related to the surface temperature.  Additional details on the relationship between surface temperature and the boundary heat flux are provided in Appendix B \ref{chap:app_b}.

After mapping, rescaling and computation of the new temperature field are complete, the FOI are passed to the CRUD simulation for all patches on the reconstruction grid.  A comparison of the CRUD results generated before and after the application of the spatial mapping procedure is provided in figure \ref{fig:crud_pre_map} and \ref{fig:crud_post_map}.

\begin{figure}[!htbp]
\centering
\begin{minipage}{.45\textwidth}
  %
  \includegraphics[width=7cm]{images/ctf_crud_orig.png}
\caption{CRUD growth on the rod \\ surface prior to HTC \\ and TKE field remapping.}
\label{fig:crud_pre_map}
\end{minipage}%
\begin{minipage}{.45\textwidth}
  %
  \includegraphics[width=7cm]{images/ctf_crud_reconstructed.png}
\caption{CRUD growth on the rod \\ surface post HTC and \\ TKE field remapping.}
\label{fig:crud_post_map}
\end{minipage}
\end{figure}

Initial results indicate that the method performs best at lower power levels when subcooled boiling does not occure.  Cases that have substantial amount of subcooled boiling tend to produce large differences between the CFD and CTF predictions which in turn reduces the predictive power gained by applying the resacaled HTC maps - essentially a garbage-in-garbage-out scinario that is of no fault to the remapping procedure.  An investigation into cause of these differences could be a subject of future work conducted in support of this Hi2Low approach.  

Though the mapping and normalizing of spatial distributions is straight forward in principal it is difficult to account for all pin configurations in the core since matching CFD and CTF solutions must be avalible in order to perform a simple map.  If corresponding CFD data is unavailabe for some portions of the thermal hydraulic conditions present in the core an interpolation procedure is required.  The effectiveness the interpolation procedure depends on the sensitivity of the spatial heat transfer and TKE distributions on the rod surface to changes in rod bundle geometry and other auxillery variables such as local power output.  If the spatial distributions are ill behaved as a function of rod orientaion and locally-averaged thermal hydraulic conditions in the core it could become prohibtively expensive to pre-compute the necissary CFD data required to build an interpolant with an acceptible level of uncertainty in the predicted quantites.  In order to quantify this uncertainty, future development of this technique will focus on multi-rod, multi-state point cases.
A key metric of success can be assertained from this study:  The Hi2Low technique which utilizes the fewest number of CFD runs to drive down uncertainties in the prediced quantites of interest (those which significantly impact CRUD growth) is a suppirior model.

\section{CFD vs CTF Comparisons}

Differences between CFD and subchannel predictions must be understood prior to constructing a Hi2Low model for enhancing CRUD preditions.  It is not possible to build an informative scale briding model upon data that is unphysical.  If excessive differences in the bulk flow conditions are predicted between the two codes, it is not useful to extract fine scale flow details from the CFD caculations and pull them into CTF models. 

\section{Copula}

Formally introduced by Sklar in 1959 \cite{Sklar1959}, a copula is a function which relates marginal probability distirbutions to a multidimensional joint distribution.  Copula provide a flexible alternative to mutidimensional gaussian based models.  Copula are utilized in this work because of their ability to capure non-gaussian dependence structure between two or more corrolated random variables, for instance temperature and the TKE at a given point on a rod's surface.  Furthermore, Sklar's theorem is used in this work in order to decompose joint distributions into a product of univariate marginal distributions and a copula function.  

Copula are leveraged in the finance industry to
predict correlated extreme value risks in credit portfolios
\cite{Geidosch2016}.  Copula have received additional attention in financial and mathematics communities since 
simpler Gaussian based dependence modeling techniques were revealed to make erroneous expected CDO portfolio loss predictions under the market conditions present in the financial crisis of
2008-2009 \cite{MacKenzie2013}, \cite{Li2000}.  Despite the widespread adoption of copula models in financial risk assesment community, only recently have copula been applied to flood risk
models \cite{Dupuis2007}, \cite{Ganguli2012}, and reliability analysis in nuclear plants
\cite{Kelly2007}.  The delayed adoption of the copula in the
engineering realm is speculated to be due to a substantial increase in computational
complexity required to construct and evaluate high dimensional copula over
incumbent Bayesian network and multidimensional gaussain based methods.  
It is trivial to fit low dimensional copula models to empirical data
using a maximum likelihood or Markov Chain Monte Carlo approach \cite{Jouini1996}.
A method for drawing corrolated samples from a copula is provided in Appendix A.

\section{Gradient Boosting}

Gradient boosting is a supervised machine learning method for classification and regression problems.
The first gradient booting algorithms were introduced by J. H. Friedman (1999) \cite{friedman2001}.  In his work, Friedman connected the technique to optimization of a cost function by gradient descent.
Nowadays, gradient boosting is ubiquitous in the machine learning community.  Gradient boosting has found use in applicatoins ranging from ecological modeling \cite{death2007} to webpage ranking problems \cite{Tyree2011}, \cite{chapelle2011}.  Particullarly in the regression setting, gradient boosted trees are higly competitive with random forests and support vector machines \cite{moisen2006}. 

Gradient boosting is an ensemble technique in which a sequence of weak learners are fit to the training data in a stagewise fashion.  Each successive weak learner is specilized to correct the errors made by the previous learners in the ensemble.  A weighted sum over all the weak learner predictions results in the final ensemble prediction.  Making a predictions from an ensemble of weak learners is analagous to taking a weighted vote from a crowd with members each producing marginally better than random results - similar a certain game show's ``ask the audiance lifeline''.  

As in the application of any machine learning technique, care must be taken to minimize overfitting the data.  If unmitigated, outliers can have a massive impact on the predictions make by the trained model.  Regularization techniques developed by Breiman and Friedman known as shrinkage and bagging reduce the importance of outliers on the boosted model \cite{breiman1996}.  In the machine learning context, regularization referes to any technique which adds additional penalizing terms to the model in order to prevent overfitting.  The general idea is related to the use of lagrange multipliers in solving constrained optimization problems.  An introduction to the gradient boosting algorithm is provided in Appendix C.

In this work, gradient boosted descission trees are used for the regression of surface TKE and temperature quantiles and classifiaction of coupla based on local core conditions.   Each descision tree plays the role of a weak learner.  A descission tree take the form of a binary tree.  Each node in the tree represents a partition in the input space that is orthogonal to an explanatory variable axis.  In each split region, a constant predictive value is assigned; typically the mean value is used in a regression setting or the most likely class label in a classification setting.  The descision tree's used in this work are formally known as classification and regression (CART) trees.  A detailed description of CART trees and their application to ecological classification problems is provided by G. Death and K. Fabricious (2000) \cite{death2000}. 

\section{Kriging Literature Review}

Kriging is popular technique used to construct surrogate models.
This was pioneered by Danie G. Krige in his master's thesis in an effort to model the spatial distribution of ore concentrations given sparse, uncertain estimates \cite{krige51}. Following kriging's introduction, the technique has propogated through the geostatistics community.  Kriging is applicable when estimates of the mean and variance of a random field are desired in between sparse training data samples [ref].  The technique has been further explored by the machine learning community under the guise of gaussian process regression.  Kriging is related to Gaussian process regression since the underlying goal of both approaches is to model the spatial autocorrelation of a random field.  

Unfortunately the Hi2Low problem under consideration does not align with the typical applications of the technique since the spatial field data is extreemly well resolved by CFD on the rod surface but is very sparse in other auxillery dimensions, such as local power level.  Usually, kriging is applied to the opposite case where the data is known at a sparse spatial resolution but auxillery predictors are known at a higher resolution.  Despite this, as discused in section \ref{chap:fw}, a regression kriging model is currently under consideration as a means interpolate between known samples of CFD spatial data fields.  Here, a brief overview of kriging applied to an engineering problem is provided.

As with any regression technique, RK requires there to be some well behaved correlation between the auxillery variables and the target variable.  For instance, if the local power level rises in the core this should result in a rise in rod surface temperatures.


