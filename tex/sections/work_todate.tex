\section{Results}

To date single pin single, single statepoint results have been produced using the proposed methodology.  Future work remains to incorperatre a time-stepping algorithm into the model. 


\section{Software Components}

A suite of software tools has been developed to post process CFD data sets into training data sets suitable for the application of supervised machine learning.   The prior work also included developing packages for gradient boosting and copula simulation.  Additionally, a package capable of supplying boundary conditions to the CRUD simulation code by drawing sampls from a gradient boosted copula model was developed.

\subsection{Data Extraction from CFD Simulations}

In order to prepair training data sets upon which to construct a Hi2Low model, a small package has been developed to aid in data extraction from STAR-CCM+ CFD simulations.  This tool was developed as a subpackage of a code called Cicada. Initially developed in pure C as a demonstrative tool for STAR-CCM+ and CRUD coupling, Cicada's capabilities have evolved to encompass a variety new technologies.  Successful incorporation of the HDF5 library and portions of Trilinos have shown it is possible to leverage powerful C/C++ tools inside STAR-CCM+ user code.  Cicada's I/O capabilities, usability and internal error handling benefited from the inclusion of these third party tools.

Significant steps have been taken to collect and distil CFD-scale CRUD and TH field data sets into useful formats.  To this end, Cicada is accompanied by a powerful suite of post processing tools which simplifies CTF-CFD comparison, statistical model inference, and interoperability with VERA-View, a CASL visualization tool. The core data assimilation capabilities rely heavily on the HDF5 library.  As a result of recent developments, Cicada is capable of exporting CFD field-data and finely resolved MAMBA results directly from a STAR-CCM+ simulation to the HDF5 format for later post processing.  

A HDF5 read capability was implemented in Cicada to allow externally generated power profiles to be applied as a thermal boundary condition in the STAR-CCM+ domain.  This feature enables loose out of memory coupling with a neutronics package. 
The axial power distribution is mapped to a CFD surface via 1D linear interpolation along the transverse axis.  Power normalization is performed to obtain the correct heat flux magnitude such that the total energy injected into the domain is equivalent to the user specified value.   Figure \ref{fig:heat_flux_ex} displays a power profile obtained from MPACT applied on the interior surface of the cladding in STAR-CCM+.

\begin{figure}[hbtp]
\centering
\includegraphics[scale=.35]{images/heat_flux_ex.png}
\caption{Power profile read from an external HDF5 file applied to the interior surface of the cladding.}
\label{fig:heat_flux_ex}
\end{figure}

The development of Cicada's HDF5 data export capabilities revealed weakness in the current tools provided by CD-Adpaco for executing C/C++ user code inside STAR-CCM+.  When writing I/O routines in user code it is desirable to leverage the parallel capabilities of the HDF5 library, however, STAR-CCM+ carries an internal dependency on a serial version (v1.8.6) of the HDF5 library.  At the time of this writing, Cicada cannot be linked against a parallel version of the library due to symbol conflicts encountered between the user specified library compiled with parallel capabilities enabled and the serial-only hdf5 library built into STAR-CCM+.
Collaboration with CD-Adapco targeted at addressing library conflicts could benefit future multiphysics coupling endeavours that leverage STAR's C-API.

STAR-CCM+ is typically executed in a parallel environment in which each MPI processes owns a subset of the CFD domain with overlapping regions for solution transfer.  Care must be observed that data from these overlapping ``halo cells'' are not included in the export.  In a parallel environment field data is first collected on the root MPI process in sequential order before being exported to the HDF5 file.  Consequently, the serial version of the HDF5 library is sufficient for performing data transfer.  The sequential MPI send-receive paradigm guarantees that the ordering of data points is identical for each exported data field: E.g. the data is consistent with respect to global cell index in the output arrays.   

The resultant HDF5 output from Cicada is written in a point wise format.  For each user specified volumetric or surface region the respective cells' volume or area are exported alongside Cartesian coordinates of the cells' centroid.  Likewise, thermal hydraulic and CRUD fields supported at the cells' centroids are written as desired.  The raw CFD solution data residing in a Cicada HDF5 output file is useful for external multiphysics applications that are compatible with point-cloud data sets.  Additionally, the availability of point cloud data in a high performance format facilitates data analysis outside of the STAR-CCM+ environment.  

\subsubsection*{CFD vs CTF Comparisons}

Differences between CFD and subchannel predictions must be understood prior to constructing a Hi2Low model for enhancing CRUD preditions.  As mentioned previously, tools included in Cicada have facilitated CFD to CTF comparisons.  Figure () illustrates a rod operating at () nominal power.

TODO: include diff plots

\subsection{Synthetic Training Data Generation}

A toolkit to overlay custom noise atop a CTF solution was developed to provide a secondary source of training data sets.  The synthetic data generation tool provides training data sets with lower computational cost than equivalent CFD calculations.  Some properties of a true CFD solution field are preserved by the tool, namely that the shape of the marginal and copula distributions change as a function of position and local thermal hydraulic conditions in the core.  The synthetic data is not to be viewed as complete substitute for CFD data since it lacks the ability to capture spatial auto-correlation in the predicted spatial fields that arise naturally from the governing PDEs.  Neighboring points on the rod surface do not exchange any TH information in this tool.  Despite the unphysical nature of the synthetic data, the tool provides a means to verify that known relationships between the explanatory variables and the copula parameters are recovered by the gradient boosted regression model.  This is possible because the user specifies these relationships up-front as inputs to the surface field sampling routines. \\

A unique blended copula model can be specified in each span providing space dependent, correlated tri-variate $f(T,\ TKE,\ q'')$  distributions.
In order to completely specify the local TH distributions conditioned on space and local TH conditions, the parameters of the marginal distributions can be made function of space and local averaged TH conditions provided by CTF.  

An exerpt of an input to generate a synthetic single pin dataset is given below:
\tiny
\begin{lstlisting}[language=XML]
{
    "pinID": 1,
    "chanID": 1,
    "averageHeatFlux": 1.2e6,
    "spans": {
              "0.0": {"model": "lower", "samples": 1000},
              "2.01": {"model": "upper", "samples": 4000},
              "2.53": {"model": "upper", "samples": 4000},
              "2.98": {"model": "upper", "samples": 4000}
    },
    "upper": {
            "0.0": {"copula":  {"family": "gauss", "params": [-0.5], "rot": 0},
                "tke": {"type": "gauss", "params": [0.001, 0.02]},
                "temp": {"type": "beta", "params": [5.0, 2.7], "loc": -9.2, "scale": 12.0},
                "bhf": {"type": "gauss", "params": [0.001, 2.6e4]}
                },
            "0.3": {"copula":  {"family": "gauss", "params": [-0.6], "rot": 0},
                "tke": {"type": "gauss", "params": [0.01, 0.008]},
                "temp": {"type": "beta", "params": [5.0, 1.7], "loc": -7.0, "scale": 8.0},
                "bhf": {"type": "gauss", "params": [0.01, 1.1e4]}
                },
            "1.0": {"copula":  {"family": "frank", "params": [4.0], "rot": 1},
                "tke": {"type": "gauss", "params": [0.01, 0.005]},
                "temp": {"type": "beta", "params": [5.0, 1.5], "loc": -4.0, "scale": 5.0},
                "bhf": {"type": "gauss", "params": [0.01, 0.9e4]}
                }
            },
    "lower": {
            "0.0": {"copula":  {"family": "gauss", "params": [-0.6]},
                "tke": {"type": "gauss", "params": [0.001, 0.0001]},
                "temp": {"type": "beta", "params": ["5.0*(t)/600.0", 5.0], "loc": -2.0, "scale": 4.0},
                "bhf": {"type": "gauss", "params": [0.01, 1.0e3]}
                },
            "1.0": {"copula":  {"family": "gauss", "params": [-0.6]},
                "tke": {"type": "gauss", "params": [0.001, 0.0002]},
                "temp": {"type": "beta", "params": [5.0, 5.0], "loc": -2.0, "scale": 4.0},
                "bhf": {"type": "gauss", "params": [0.01, 1.0e3]}
                }
            }
}
\end{lstlisting}
\normalsize

The corrosponding sampled temperature and TKE distributions for single pin are provided in figures () and (). \\

Figure here \\

The variance of the temperature and TKE margins was increased immediately following a spacer grid since one would expect the grid to create extra turbulence in these regions.  Additionally, the correlation coefficient of the copula model is adjusted depending on the distance to the nearest spacer grid to simulate a decrease in the concordance between surface temperature and surface shear stress present in real CFD data sets following a spacer grid. 
The synthetic data generation package \emph{ctfPurt} is available at \url{https://github.com/wgurecky/ctfPurt.git}.

\subsection{Gradient Boosting Toolkit}

A gradient boosting library was developed in the python programming language.  This package provides an easily extensible loss function class, allowing the user to implement arbitrary loss functions in the gradient boosting framework.  As required by the proposed work, both quantile and least squares loss functions are included.  The package is applicable to both regression and classification problems.  CART tree construction controls are also provided allowing fine grained control over the weak learners.
The library interface was constructed to be similar to Scikit-learn's gradient boosting API so that the newly developed boosting algorithms can stand as drop in replacements for those available in Scikit-learn.

The gradient boosting package \emph{pCRTree} is available at \url{https://github.com/wgurecky/pCRTree.git}.

\subsection{Copula Toolkit}

For copula simulation, the CDvine toolkit (GPLv3 licensed) is available for the R programming language. This packages does not implement all rotations of copula making it burdensome to handle negative dependence structures out-of-the-box.  Furthermore, the maximum likelihood fitting method included in CDVine does not allow the user to specify sample weights.

To circumvent these deficiencies and potential license compatibility issues with VERA, a new copula toolkit was developed in python and is BSD3 licensed.
Careful attention was paid to develop a flexible abstract copula class which enables custom coupla functions to be specified.  Importantly, all copula rotations are supported by default allowing one to model positive and negative dependence structures without duplication of code.
Canonical vine-copula construction and sampling algorithms are included in this package to handle the decomposition of arbitrary joint density functions of any dimension.
Copula parameters can be determined by a weighted maximum likelihood fit to empirically supplied data with included sample weights or by specifying a rank correlation coefficient in the case of Archimedean copula.  In the proposed Hi2Low work, both capabilities are leveraged.

The \emph{StarVine} copula package and documentation is available at \url{https://github.com/wgurecky/StarVine.git}.

\subsection{Python Interfaces to CRUD Codes}

As part of this work, pythonic interfaces were developed for both the legacy CASL CRUD tool known as MAMBA1D and the state-of-the art CRUD package, Mongoose.  The python wrappers to these Fortran codes facilitate rapid prototyping of Hi2Low procedures which provide boundary conditions to the CRUD codes.  Additionally, the high level interface simplifies the proccess of orchestrating large CRUD sensitivity studies.

The python wrappers are available for download from casl-dev at \url{git@casl-dev:collaboration/MAMBA/pyMAMBA} and \url{git@casl-dev:collaboration/Mongoose/pygoose}.

\subsection{Hi2Low Crud Growth Code}

Finally, a package that leverages all the aforementioned tools was developed.  This high level package is the primary user facing result of the proposed work - though it should be noted this package is heavily dependent on CRUD simulation, copula construction, and gradient boosting technologies.
This package orchestrates the construction and evaluation of GBRTs which provide boundary conditions for the CRUD simulation.
Currently, single pin, single statepoint simulation is implemented with future work focused on multi pin and multi state point execution.

The High2Low crud growth package and documentation is available at \url{https://github.com/wgurecky/crudBoost.git}.
